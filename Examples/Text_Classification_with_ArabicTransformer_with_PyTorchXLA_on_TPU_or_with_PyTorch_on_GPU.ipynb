{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification with ArabicTransformer with PyTorchXLA on TPU or with PyTorch on GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQvQLZH99PFw"
      },
      "source": [
        "**Text Classification with ArabicTransformer and TPU**\n",
        "\n",
        "*   First, you need to activate TPU by going to Runtime-> Change RunTime Type -> TPU .\n",
        "\n",
        "*   This example was tested with HuggingFace Transformer Library version v4.11.2 . If you experience any issue roll back to this version.\n",
        "\n",
        "*   This example uses PyTorchXLA, a library that allows you to use PyTorch code on TPU by having PyTorchXLA in the middle. You may experience that the pre-processing of the dataset is slow if you run the code for the first time, but this is just for the first time. If you change the batch size, the pre-processing again will be slow. So try to fix the batch size every time you do a grid search for the best hyperparameters. \n",
        "\n",
        "*   In our paper, we use the original implementation of funnel transformer (PyTorch) (https://github.com/laiguokun/Funnel-Transformer) and V100 GPU, which is no longer provided for Google Colab Pro users. We will update you later on our modified code of the Funnel Transfomer library. However, in the meantime, you need to find the best hyperparameters here and dont rely in our setting in this notebook since the implementation is different from our paper. However, our current set of hyperparameters in this example is still close to what we reported in our paper. You may also get better results with our model than what we reported if you extend the grid search (:\n",
        "\n",
        "* You can easily run this code on GPU with O2 mixed precision by just changing the runtime to GPU and removing this line from fine-tuning code \n",
        "\n",
        "```\n",
        "!python /content/transformers/examples/pytorch/xla_spawn.py --num_cores=8 transformers/examples/pytorch/text-classification/run_glue.py \n",
        "```\n",
        "\n",
        "with \n",
        "\n",
        "```\n",
        "!python transformers/examples/pytorch/text-classification/run_glue.py\n",
        "```\n",
        "\n",
        "\n",
        "* The new pytorch library >1.6 allow you to use Automatic Mixed Precision (AMP)  without APEX since its part of the native PyTroch library. \n",
        "\n",
        "\n",
        "*   This example is based on GLUE fine-tuning task example from huggingface team but it can work with any text classification task and can be used to fine-tune any Arabic Language Model that was uploaded to HuggingFace Hub here https://huggingface.co/models . A text classification task is where we have a sentence and a label like sentiment analysis tasks. You just need to name the header of first sentence that you need to classify to sentence1 and label to \"label\" colmun. If you want to classify two sentences, then name the first sentence as sentence1 and the other one to sentence2 .\n",
        "\n",
        "*   When you use PyTorchXLA, then you should be aware the batch size will be batch_size*8 since we have 8 cores on the TPU. In this example, we choose a batch size of 4 to get the final batch size of 32.\n",
        "\n",
        "*   We did not include language models that use pre-segmentation (FARASA), such as AraBERTv2, in the list of models below. You can do the pre-segmentation part from your own side using codes that AUB Mind published here https://github.com/aub-mind/arabert. Then use our code to fine-tune AraBERTv2 or similar models.\n",
        "\n",
        "*   If the model scale is changed (small, base, large) or the architecture is different (Funnel, BERT, ELECTRA, ALBERT), you need to change your hyperparameters. Evaluating all models using the same hyperparameters across different scales and architectures is bad practice to report results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaihe27TrDHX",
        "outputId": "8bf45495-b101-4241-826b-be743326e7c9"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip3 install -e transformers \n",
        "!pip3 install -r transformers/examples/pytorch/text-classification/requirements.txt\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 85569, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 85569 (delta 8), reused 14 (delta 3), pack-reused 85541\u001b[K\n",
            "Receiving objects: 100% (85569/85569), 68.44 MiB | 34.44 MiB/s, done.\n",
            "Resolving deltas: 100% (61496/61496), done.\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (3.0.12)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 68.7 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.8.1)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.12.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.12.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.0.dev0\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting datasets>=1.8.0\n",
            "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r transformers/examples/pytorch/text-classification/requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r transformers/examples/pytorch/text-classification/requirements.txt (line 5)) (0.22.2.post1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r transformers/examples/pytorch/text-classification/requirements.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from -r transformers/examples/pytorch/text-classification/requirements.txt (line 7)) (1.9.0+cu102)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 68.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (4.8.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (0.0.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (2.23.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 75.9 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 78.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (0.70.12.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (0.3.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->-r transformers/examples/pytorch/text-classification/requirements.txt (line 7)) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (2021.5.30)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate->-r transformers/examples/pytorch/text-classification/requirements.txt (line 1)) (5.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r transformers/examples/pytorch/text-classification/requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r transformers/examples/pytorch/text-classification/requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (21.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 78.5 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 77.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r transformers/examples/pytorch/text-classification/requirements.txt (line 2)) (2.8.2)\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, sentencepiece, datasets, accelerate\n",
            "Successfully installed accelerate-0.5.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.12.1 fsspec-2021.9.0 multidict-5.1.0 sentencepiece-0.1.96 xxhash-2.0.2 yarl-1.6.3\n",
            "Collecting torch-xla==1.9\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 149.9 MB 16 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.35.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Installing collected packages: google-api-python-client, torch-xla, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.283 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJsmWd35r8Xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516ae82d-0e17-4e5b-91ab-b96f723d9be2"
      },
      "source": [
        "import pandas as pd\n",
        "!rm -r /content/data\n",
        "!mkdir -p data/raw/scarcasmv2\n",
        "!mkdir -p data/scarcasmv2\n",
        "!wget -O data/raw/scarcasmv2/dev.csv https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/testing_data.csv\n",
        "!wget -O data/raw/scarcasmv2/train.csv https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv\n",
        "df = pd.read_csv(r'data/raw/scarcasmv2/train.csv', header=0,escapechar='\\n',usecols = [0,2],names=[\"sentence1\", \"label\"])\n",
        "df.to_csv('data/scarcasmv2/train.csv',index=False)\n",
        "df.to_csv('data/scarcasmv2/train.tsv',sep='\\t',index=False)\n",
        "df = pd.read_csv(r'data/raw/scarcasmv2/dev.csv', header=0, escapechar='\\n',usecols = [0,2],names=[\"sentence1\", \"label\"])\n",
        "df.to_csv('data/scarcasmv2/dev.csv',index=False)\n",
        "df.to_csv('data/scarcasmv2/dev.tsv',sep='\\t',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/data': No such file or directory\n",
            "--2021-10-01 23:41:58--  https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/testing_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 585081 (571K) [text/plain]\n",
            "Saving to: ‘data/raw/scarcasmv2/dev.csv’\n",
            "\n",
            "data/raw/scarcasmv2 100%[===================>] 571.37K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-10-01 23:41:58 (13.8 MB/s) - ‘data/raw/scarcasmv2/dev.csv’ saved [585081/585081]\n",
            "\n",
            "--2021-10-01 23:41:58--  https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2393168 (2.3M) [text/plain]\n",
            "Saving to: ‘data/raw/scarcasmv2/train.csv’\n",
            "\n",
            "data/raw/scarcasmv2 100%[===================>]   2.28M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-10-01 23:41:58 (22.8 MB/s) - ‘data/raw/scarcasmv2/train.csv’ saved [2393168/2393168]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4zvQh9IjlOz"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score,classification_report,accuracy_score\n",
        "def calc_scarcasm(y_pred,y_true):\n",
        "  y_pred=pd.read_csv(y_pred, sep='\\t',header=None,usecols=[1] )\n",
        "  y_true=pd.read_csv(y_true,usecols=[1],header=None)\n",
        "  print(\"Accur Score:\",accuracy_score(y_true, y_pred)*100)\n",
        "  print(\"F1 PN Score:\",f1_score(y_true, y_pred,labels=['NEG','POS'],average=\"macro\")*100)\n",
        "  print(\"########################### Full Report ###########################\")\n",
        "  print(classification_report(y_true, y_pred,digits=4,labels=['NEG','POS'] ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3cJQVJNKfUA"
      },
      "source": [
        "# **ArabicTransformer Small (B4-4-4)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Km3xMPqraxO",
        "outputId": "378420d6-ed1f-44e6-e189-10585319c7c3"
      },
      "source": [
        "import os\n",
        "model= \"sultan/ArabicTransformer-small\" #@param [\"sultan/ArabicTransformer-small\",\"sultan/ArabicTransformer-intermediate\",\"sultan/ArabicTransformer-large\",\"aubmindlab/araelectra-base-discriminator\",\"asafaya/bert-base-arabic\",\"aubmindlab/bert-base-arabertv02\",\"aubmindlab/bert-base-arabert\", \"aubmindlab/bert-base-arabertv01\",\"kuisailab/albert-base-arabic\",\"aubmindlab/bert-large-arabertv02\"]\n",
        "task= \"scarcasmv2\" #@param [\"scarcasmv2\"]\n",
        "seed= \"42\" #@param [\"42\", \"123\", \"1234\",\"12345\",\"666\"]\n",
        "batch_size = 4 #@param {type:\"slider\", min:4, max:128, step:4}\n",
        "learning_rate = \"3e-5\"#@param [\"1e-4\", \"3e-4\", \"1e-5\",\"3e-5\",\"5e-5\",\"7e-5\"]\n",
        "epochs_num = 2 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "max_seq_length= \"256\" #@param [\"128\", \"256\", \"384\",\"512\"]\n",
        "os.environ['batch_size'] = str(batch_size)\n",
        "os.environ['learning_rate'] = str(learning_rate)\n",
        "os.environ['epochs_num'] = str(epochs_num)\n",
        "os.environ['task'] = str(task)\n",
        "os.environ['model'] = str(model)\n",
        "os.environ['max_seq_length'] = str(max_seq_length)\n",
        "os.environ['seed'] = str(seed)\n",
        "!python /content/transformers/examples/pytorch/xla_spawn.py --num_cores=8 transformers/examples/pytorch/text-classification/run_glue.py --model_name_or_path $model \\\n",
        "--train_file data/$task/train.csv \\\n",
        "--validation_file data/$task/dev.csv \\\n",
        "--test_file data/$task/dev.csv \\\n",
        "--output_dir output_dir/$task \\\n",
        "--overwrite_cache \\\n",
        "--seed $seed \\\n",
        "--overwrite_output_dir \\\n",
        "--logging_steps 1000000 \\\n",
        "--max_seq_length $max_seq_length \\\n",
        "--per_device_train_batch_size $batch_size \\\n",
        "--learning_rate $learning_rate \\\n",
        "--warmup_ratio 0.1 \\\n",
        "--num_train_epochs $epochs_num \\\n",
        "--save_steps 50000 \\\n",
        "--do_train \\\n",
        "--do_predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.9\n",
            "10/02/2021 03:45:04 - WARNING - run_glue - Process rank: -1, device: xla:1, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "10/02/2021 03:45:04 - INFO - run_glue - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output_dir/scarcasmv2/runs/Oct02_03-45-04_413153eeb4a3,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=1000000,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "output_dir=output_dir/scarcasmv2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output_dir/scarcasmv2,\n",
            "save_on_each_node=False,\n",
            "save_steps=50000,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=8,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "10/02/2021 03:45:04 - INFO - run_glue - load a local file for train: data/scarcasmv2/train.csv\n",
            "10/02/2021 03:45:04 - INFO - run_glue - load a local file for validation: data/scarcasmv2/dev.csv\n",
            "10/02/2021 03:45:04 - INFO - run_glue - load a local file for test: data/scarcasmv2/dev.csv\n",
            "10/02/2021 03:45:04 - WARNING - datasets.builder - Using custom data configuration default-80714e2580e076a5\n",
            "10/02/2021 03:45:04 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "10/02/2021 03:45:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff\n",
            "10/02/2021 03:45:04 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "10/02/2021 03:45:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff\n",
            "100% 3/3 [00:00<00:00, 761.35it/s]\n",
            "10/02/2021 03:45:04 - WARNING - run_glue - Process rank: -1, device: xla:0, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "[INFO|configuration_utils.py:583] 2021-10-02 03:45:04,591 >> loading configuration file https://huggingface.co/sultan/ArabicTransformer-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d0327c837b96e7000e54f490c9d51d7ec20919038c8122691e166bcfe7b59465.bbb85e253d29869a3093bcdf61b488f61e9325aae8446e08e85c9369efc33fa9\n",
            "[INFO|configuration_utils.py:620] 2021-10-02 03:45:04,593 >> Model config FunnelConfig {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"architectures\": [\n",
            "    \"FunnelModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_type\": \"relative_shift\",\n",
            "  \"block_repeats\": [\n",
            "    1,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"block_sizes\": [\n",
            "    4,\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.1,\n",
            "  \"initializer_std\": null,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-09,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"funnel\",\n",
            "  \"n_head\": 12,\n",
            "  \"num_decoder_layers\": 2,\n",
            "  \"pool_q_only\": true,\n",
            "  \"pooling_type\": \"mean\",\n",
            "  \"rel_attn_type\": \"factorized\",\n",
            "  \"separate_cls\": true,\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"truncate_seq\": true,\n",
            "  \"type_vocab_size\": 3,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "10/02/2021 03:45:04 - WARNING - datasets.builder - Using custom data configuration default-80714e2580e076a5\n",
            "10/02/2021 03:45:04 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "100% 3/3 [00:00<00:00, 710.38it/s]\n",
            "[INFO|tokenization_auto.py:334] 2021-10-02 03:45:04,717 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "10/02/2021 03:45:04 - WARNING - run_glue - Process rank: -1, device: xla:0, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "10/02/2021 03:45:04 - WARNING - run_glue - Process rank: -1, device: xla:0, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "[INFO|configuration_utils.py:583] 2021-10-02 03:45:04,850 >> loading configuration file https://huggingface.co/sultan/ArabicTransformer-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d0327c837b96e7000e54f490c9d51d7ec20919038c8122691e166bcfe7b59465.bbb85e253d29869a3093bcdf61b488f61e9325aae8446e08e85c9369efc33fa9\n",
            "[INFO|configuration_utils.py:620] 2021-10-02 03:45:04,850 >> Model config FunnelConfig {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"architectures\": [\n",
            "    \"FunnelModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_type\": \"relative_shift\",\n",
            "  \"block_repeats\": [\n",
            "    1,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"block_sizes\": [\n",
            "    4,\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"initializer_range\": 0.1,\n",
            "  \"initializer_std\": null,\n",
            "  \"layer_norm_eps\": 1e-09,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"funnel\",\n",
            "  \"n_head\": 12,\n",
            "  \"num_decoder_layers\": 2,\n",
            "  \"pool_q_only\": true,\n",
            "  \"pooling_type\": \"mean\",\n",
            "  \"rel_attn_type\": \"factorized\",\n",
            "  \"separate_cls\": true,\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"truncate_seq\": true,\n",
            "  \"type_vocab_size\": 3,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "10/02/2021 03:45:04 - WARNING - datasets.builder - Using custom data configuration default-80714e2580e076a5\n",
            "10/02/2021 03:45:04 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "100% 3/3 [00:00<00:00, 710.86it/s]\n",
            "10/02/2021 03:45:05 - WARNING - datasets.builder - Using custom data configuration default-80714e2580e076a5\n",
            "10/02/2021 03:45:05 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "100% 3/3 [00:00<00:00, 763.16it/s]\n",
            "10/02/2021 03:45:05 - WARNING - run_glue - Process rank: -1, device: xla:0, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "10/02/2021 03:45:05 - WARNING - datasets.builder - Using custom data configuration default-80714e2580e076a5\n",
            "10/02/2021 03:45:05 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "100% 3/3 [00:00<00:00, 749.56it/s]\n",
            "10/02/2021 03:45:05 - WARNING - run_glue - Process rank: -1, device: xla:0, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "10/02/2021 03:45:05 - WARNING - datasets.builder - Using custom data configuration default-80714e2580e076a5\n",
            "10/02/2021 03:45:05 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "100% 3/3 [00:00<00:00, 717.75it/s]\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-02 03:45:05,609 >> loading file https://huggingface.co/sultan/ArabicTransformer-small/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/d4a0e7e66d6b672cf2c9c8503d206cae3c57f6fc5148138618bc3c481db82749.6a31ca05dca1bd2edaae0d695553d412efcfc6d760947c86cf82b8be51335d3a\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-02 03:45:05,609 >> loading file https://huggingface.co/sultan/ArabicTransformer-small/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-02 03:45:05,609 >> loading file https://huggingface.co/sultan/ArabicTransformer-small/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-02 03:45:05,609 >> loading file https://huggingface.co/sultan/ArabicTransformer-small/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-02 03:45:05,609 >> loading file https://huggingface.co/sultan/ArabicTransformer-small/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:583] 2021-10-02 03:45:05,734 >> loading configuration file https://huggingface.co/sultan/ArabicTransformer-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d0327c837b96e7000e54f490c9d51d7ec20919038c8122691e166bcfe7b59465.bbb85e253d29869a3093bcdf61b488f61e9325aae8446e08e85c9369efc33fa9\n",
            "[INFO|configuration_utils.py:620] 2021-10-02 03:45:05,735 >> Model config FunnelConfig {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"architectures\": [\n",
            "    \"FunnelModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_type\": \"relative_shift\",\n",
            "  \"block_repeats\": [\n",
            "    1,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"block_sizes\": [\n",
            "    4,\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"initializer_range\": 0.1,\n",
            "  \"initializer_std\": null,\n",
            "  \"layer_norm_eps\": 1e-09,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"funnel\",\n",
            "  \"n_head\": 12,\n",
            "  \"num_decoder_layers\": 2,\n",
            "  \"pool_q_only\": true,\n",
            "  \"pooling_type\": \"mean\",\n",
            "  \"rel_attn_type\": \"factorized\",\n",
            "  \"separate_cls\": true,\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"truncate_seq\": true,\n",
            "  \"type_vocab_size\": 3,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "10/02/2021 03:45:05 - WARNING - run_glue - Process rank: -1, device: xla:0, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "[INFO|tokenization_utils.py:376] 2021-10-02 03:45:05,794 >> Adding <s> to the vocabulary\n",
            "[INFO|tokenization_utils.py:376] 2021-10-02 03:45:05,794 >> Adding </s> to the vocabulary\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:05,795 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "10/02/2021 03:45:05 - WARNING - run_glue - Process rank: -1, device: xla:0, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "[INFO|configuration_utils.py:583] 2021-10-02 03:45:05,919 >> loading configuration file https://huggingface.co/sultan/ArabicTransformer-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d0327c837b96e7000e54f490c9d51d7ec20919038c8122691e166bcfe7b59465.bbb85e253d29869a3093bcdf61b488f61e9325aae8446e08e85c9369efc33fa9\n",
            "[INFO|configuration_utils.py:620] 2021-10-02 03:45:05,920 >> Model config FunnelConfig {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"architectures\": [\n",
            "    \"FunnelModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_type\": \"relative_shift\",\n",
            "  \"block_repeats\": [\n",
            "    1,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"block_sizes\": [\n",
            "    4,\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"initializer_range\": 0.1,\n",
            "  \"initializer_std\": null,\n",
            "  \"layer_norm_eps\": 1e-09,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"funnel\",\n",
            "  \"n_head\": 12,\n",
            "  \"num_decoder_layers\": 2,\n",
            "  \"pool_q_only\": true,\n",
            "  \"pooling_type\": \"mean\",\n",
            "  \"rel_attn_type\": \"factorized\",\n",
            "  \"separate_cls\": true,\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"truncate_seq\": true,\n",
            "  \"type_vocab_size\": 3,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:05,954 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "10/02/2021 03:45:05 - WARNING - datasets.builder - Using custom data configuration default-80714e2580e076a5\n",
            "10/02/2021 03:45:06 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "100% 3/3 [00:00<00:00, 679.90it/s]\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:06,033 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "10/02/2021 03:45:06 - WARNING - datasets.builder - Using custom data configuration default-80714e2580e076a5\n",
            "10/02/2021 03:45:06 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "100% 3/3 [00:00<00:00, 756.32it/s]\n",
            "[INFO|modeling_utils.py:1323] 2021-10-02 03:45:06,133 >> loading weights file https://huggingface.co/sultan/ArabicTransformer-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b128ebd78ce7c45c638d5ac82dd580dff5d0de5c5804e5d29462c2cb98cdd05c.8aee5cc517602c5078c5165bcf931dd68b460aa6aa09172afa39b132d8aba8a2\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:06,185 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:06,338 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:06,422 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:06,494 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:06,585 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:06,674 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:06,834 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:06,991 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:07,152 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:07,456 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:07,499 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:07,633 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|tokenization_utils_base.py:1933] 2021-10-02 03:45:07,658 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|modeling_utils.py:1580] 2021-10-02 03:45:08,137 >> Some weights of the model checkpoint at sultan/ArabicTransformer-small were not used when initializing FunnelForSequenceClassification: ['decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.1.attention.layer_norm.bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.1.attention.seg_embed']\n",
            "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1591] 2021-10-02 03:45:08,137 >> Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at sultan/ArabicTransformer-small and are newly initialized: ['classifier.linear_hidden.bias', 'classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/13 [00:00<?, ?ba/s]10/02/2021 03:45:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-703090b96eaeb2ae.arrow\n",
            "[WARNING|modeling_utils.py:1580] 2021-10-02 03:45:08,492 >> Some weights of the model checkpoint at sultan/ArabicTransformer-small were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.weight', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias']\n",
            "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1591] 2021-10-02 03:45:08,492 >> Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at sultan/ArabicTransformer-small and are newly initialized: ['classifier.linear_hidden.bias', 'classifier.linear_out.bias', 'classifier.linear_out.weight', 'classifier.linear_hidden.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:  46% 6/13 [00:00<00:01,  6.75ba/s][WARNING|modeling_utils.py:1580] 2021-10-02 03:45:09,234 >> Some weights of the model checkpoint at sultan/ArabicTransformer-small were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.layer_norm.bias', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.weight']\n",
            "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1591] 2021-10-02 03:45:09,239 >> Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at sultan/ArabicTransformer-small and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_hidden.bias', 'classifier.linear_out.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[WARNING|modeling_utils.py:1580] 2021-10-02 03:45:09,245 >> Some weights of the model checkpoint at sultan/ArabicTransformer-small were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.attention.layer_norm.bias', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.0.attention.layer_norm.weight']\n",
            "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1591] 2021-10-02 03:45:09,245 >> Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at sultan/ArabicTransformer-small and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_hidden.bias', 'classifier.linear_out.bias', 'classifier.linear_out.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:  69% 9/13 [00:01<00:00,  7.81ba/s][WARNING|modeling_utils.py:1580] 2021-10-02 03:45:09,615 >> Some weights of the model checkpoint at sultan/ArabicTransformer-small were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.1.attention.layer_norm.bias', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.bias']\n",
            "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1591] 2021-10-02 03:45:09,615 >> Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at sultan/ArabicTransformer-small and are newly initialized: ['classifier.linear_out.bias', 'classifier.linear_hidden.bias', 'classifier.linear_out.weight', 'classifier.linear_hidden.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100% 13/13 [00:01<00:00,  7.57ba/s]\n",
            "[WARNING|modeling_utils.py:1580] 2021-10-02 03:45:09,904 >> Some weights of the model checkpoint at sultan/ArabicTransformer-small were not used when initializing FunnelForSequenceClassification: ['decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.attention.layer_norm.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.attention.layer_norm.bias']\n",
            "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1591] 2021-10-02 03:45:09,904 >> Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at sultan/ArabicTransformer-small and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_hidden.bias', 'classifier.linear_out.weight', 'classifier.linear_out.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/3 [00:00<?, ?ba/s]10/02/2021 03:45:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-1fc873b71958887a.arrow\n",
            "Running tokenizer on dataset:  67% 2/3 [00:00<00:00,  5.79ba/s][WARNING|modeling_utils.py:1580] 2021-10-02 03:45:10,331 >> Some weights of the model checkpoint at sultan/ArabicTransformer-small were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.layer_norm.bias', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.0.attention.v_head.bias']\n",
            "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1591] 2021-10-02 03:45:10,332 >> Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at sultan/ArabicTransformer-small and are newly initialized: ['classifier.linear_out.bias', 'classifier.linear_hidden.weight', 'classifier.linear_hidden.bias', 'classifier.linear_out.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  6.80ba/s]\n",
            "[WARNING|modeling_utils.py:1580] 2021-10-02 03:45:10,371 >> Some weights of the model checkpoint at sultan/ArabicTransformer-small were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.1.attention.layer_norm.bias']\n",
            "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1591] 2021-10-02 03:45:10,371 >> Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at sultan/ArabicTransformer-small and are newly initialized: ['classifier.linear_hidden.bias', 'classifier.linear_out.bias', 'classifier.linear_out.weight', 'classifier.linear_hidden.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/3 [00:00<?, ?ba/s]10/02/2021 03:45:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-80714e2580e076a5/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-51d62bc4abe725b6.arrow\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00, 13.81ba/s]\n",
            "10/02/2021 03:45:10 - INFO - run_glue - Sample 10476 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2, 1688, 8678, 1949, 10595, 5771, 1696, 1648, 15971, 10707, 1630, 3559, 3890, 38885, 2780, 2238, 33900, 1042, 1651, 8678, 20078, 40237, 2611, 1974, 1820, 7493, 3407, 24357, 1007, 2234, 10595, 1820, 19128, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': 'لا اعترف بدستور مسلوق على عجل خوفا من حكم المحكمة ببطلان اللجنة التى كتبته0لا اعترف برئيس يعطل القانون رئيسكم ديكتاتور ودستوركم باطل', 'token_type_ids': [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "10/02/2021 03:45:10 - INFO - run_glue - Sample 1824 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2, 1831, 2605, 10923, 4973, 2237, 1807, 2095, 1630, 23771, 14486, 2123, 28022, 16101, 33087, 1053, 7, 5635, 41, 13289, 7, 8548, 41, 1825, 41, 45048, 30, 13, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': 'هو انا عشان بعمل الصح وبغير من نفسى للأحسن ابقى كده معقده #مجتمع_مريض #حافظ_مش_فاهم :)', 'token_type_ids': [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "10/02/2021 03:45:10 - INFO - run_glue - Sample 409 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2, 2818, 1653, 1648, 23750, 1936, 33971, 3261, 26392, 17880, 5176, 12801, 1648, 11440, 19946, 10323, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': 'رموا على جمعه وجابو خالد الجندى الاحتفالات الدينية حرام على مرسي حلال للجيش', 'token_type_ids': [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Running tokenizer on dataset:   0% 0/13 [00:00<?, ?ba/s]10/02/2021 03:45:10 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy\n",
            "10/02/2021 03:45:10 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b\n",
            "10/02/2021 03:45:10 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/accuracy/accuracy.py to /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.py\n",
            "10/02/2021 03:45:10 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/accuracy/dataset_infos.json\n",
            "10/02/2021 03:45:10 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.json\n",
            "Running tokenizer on dataset: 100% 13/13 [00:01<00:00,  8.72ba/s]\n",
            "Running tokenizer on dataset: 100% 13/13 [00:01<00:00,  8.74ba/s]\n",
            "Running tokenizer on dataset: 100% 13/13 [00:01<00:00,  8.40ba/s]\n",
            "Running tokenizer on dataset: 100% 13/13 [00:01<00:00,  8.40ba/s]\n",
            "Running tokenizer on dataset: 100% 13/13 [00:01<00:00,  8.39ba/s]\n",
            "Running tokenizer on dataset: 100% 13/13 [00:01<00:00,  8.37ba/s]\n",
            "Running tokenizer on dataset: 100% 13/13 [00:01<00:00,  8.34ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  6.94ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  7.15ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  6.50ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  6.48ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  6.58ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  6.41ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  6.36ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00, 11.72ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  9.89ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00, 10.04ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  9.08ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  8.95ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  8.27ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  4.45ba/s]\n",
            "[INFO|trainer.py:541] 2021-10-02 03:45:32,684 >> The following columns in the training set  don't have a corresponding argument in `FunnelForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:1196] 2021-10-02 03:45:32,698 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-02 03:45:32,698 >>   Num examples = 12548\n",
            "[INFO|trainer.py:1198] 2021-10-02 03:45:32,698 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:1199] 2021-10-02 03:45:32,698 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1200] 2021-10-02 03:45:32,698 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1201] 2021-10-02 03:45:32,698 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-02 03:45:32,698 >>   Total optimization steps = 786\n",
            "100% 786/786 [13:36<00:00,  1.04s/it][INFO|trainer.py:1401] 2021-10-02 03:59:09,030 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 816.4567, 'train_samples_per_second': 30.738, 'train_steps_per_second': 0.963, 'train_loss': 0.6949639939169847, 'epoch': 2.0}\n",
            "100% 786/786 [13:36<00:00,  1.04s/it]\n",
            "[INFO|trainer.py:1957] 2021-10-02 03:59:09,157 >> Saving model checkpoint to output_dir/scarcasmv2\n",
            "[INFO|configuration_utils.py:413] 2021-10-02 03:59:09,210 >> Configuration saved in output_dir/scarcasmv2/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-02 03:59:13,145 >> Model weights saved in output_dir/scarcasmv2/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-02 03:59:13,146 >> tokenizer config file saved in output_dir/scarcasmv2/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-02 03:59:13,146 >> Special tokens file saved in output_dir/scarcasmv2/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        2.0\n",
            "  train_loss               =      0.695\n",
            "  train_runtime            = 0:13:36.45\n",
            "  train_samples            =      12548\n",
            "  train_samples_per_second =     30.738\n",
            "  train_steps_per_second   =      0.963\n",
            "10/02/2021 03:59:13 - INFO - run_glue - *** Predict ***\n",
            "[INFO|trainer.py:541] 2021-10-02 03:59:13,251 >> The following columns in the test set  don't have a corresponding argument in `FunnelForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2235] 2021-10-02 03:59:13,253 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2237] 2021-10-02 03:59:13,253 >>   Num examples = 3000\n",
            "[INFO|trainer.py:2240] 2021-10-02 03:59:13,253 >>   Batch size = 8\n",
            "100% 47/47 [00:26<00:00,  1.71it/s]10/02/2021 03:59:41 - INFO - run_glue - ***** Predict results None *****\n",
            "[INFO|modelcard.py:446] 2021-10-02 03:59:41,623 >> Dropping the following result as it does not have all the necessary field:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}}\n",
            "100% 47/47 [00:27<00:00,  1.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcub9L6FjjVA",
        "outputId": "1d31a5df-4488-4284-bca9-fa2542f0bc8a"
      },
      "source": [
        "calc_scarcasm('/content/output_dir/scarcasmv2/predict_results_None.txt','/content/data/scarcasmv2/dev.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accur Score: 69.97667444185271\n",
            "F1 PN Score: 72.46443739729156\n",
            "########################### Full Report ###########################\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NEG     0.7741    0.8050    0.7892      1677\n",
            "         POS     0.5886    0.7513    0.6600       575\n",
            "\n",
            "   micro avg     0.7191    0.7913    0.7535      2252\n",
            "   macro avg     0.6813    0.7782    0.7246      2252\n",
            "weighted avg     0.7267    0.7913    0.7563      2252\n",
            "\n"
          ]
        }
      ]
    }
  ]
}