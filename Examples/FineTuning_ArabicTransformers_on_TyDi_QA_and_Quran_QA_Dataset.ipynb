{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to have at least Tesla T4 GPU for faster finetuning. Google colab pro offers T4 GPU."
      ],
      "metadata": {
        "id": "X13ZM00LrsUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRa4fNJol2rv",
        "outputId": "14404af5-bf50-4a91-df6a-57cdf7811ff7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb 19 16:52:39 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    31W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nbCTAfpnYLCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d53e056-4b23-4c7d-ef2a-9a820463f53e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.26.1\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.1) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.1) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 3271, done.\u001b[K\n",
            "remote: Counting objects: 100% (3271/3271), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2711/2711), done.\u001b[K\n",
            "remote: Total 3271 (delta 1008), reused 1124 (delta 511), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3271/3271), 11.37 MiB | 10.86 MiB/s, done.\n",
            "Resolving deltas: 100% (1008/1008), done.\n",
            "Note: switching to 'ae54e3c3b18bac0832ad62ea9b896dfd52a09850'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, multiprocess, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.9.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m271.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.25.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.9.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2023.1.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from evaluate) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from farasapy) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from farasapy) (2.25.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->farasapy) (1.26.14)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from pyarabic) (1.15.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzysearch\n",
            "  Downloading fuzzysearch-0.7.3.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=19.3 in /usr/local/lib/python3.8/dist-packages (from fuzzysearch) (22.2.0)\n",
            "Building wheels for collected packages: fuzzysearch\n",
            "  Building wheel for fuzzysearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzysearch: filename=fuzzysearch-0.7.3-cp38-cp38-linux_x86_64.whl size=364022 sha256=58b2f6171762b746637b7777ccbdddd23823e4f67fc08607efecf727b12402db\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/e6/eb/4c4b250a0d6562161dcb8667e2cb07a5e20b257fcb75e50b04\n",
            "Successfully built fuzzysearch\n",
            "Installing collected packages: fuzzysearch\n",
            "Successfully installed fuzzysearch-0.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting camel_tools\n",
            "  Downloading camel_tools-1.4.1-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from camel_tools) (2.25.1)\n",
            "Requirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from camel_tools) (4.26.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from camel_tools) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.8/dist-packages (from camel_tools) (1.13.1+cu116)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.8/dist-packages (from camel_tools) (0.5.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from camel_tools) (0.3.6)\n",
            "Requirement already satisfied: pyrsistent in /usr/local/lib/python3.8/dist-packages (from camel_tools) (0.19.3)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.8/dist-packages (from camel_tools) (5.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from camel_tools) (1.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from camel_tools) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from camel_tools) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from camel_tools) (1.3.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from camel_tools) (0.8.10)\n",
            "Collecting camel-kenlm\n",
            "  Downloading camel-kenlm-2021.12.27.tar.gz (418 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.2/418.2 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from camel_tools) (4.64.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from camel_tools) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->camel_tools) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.2->camel_tools) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.2->camel_tools) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.2->camel_tools) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.2->camel_tools) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.2->camel_tools) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.2->camel_tools) (6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->camel_tools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->camel_tools) (2022.7.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->camel_tools) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->camel_tools) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->camel_tools) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->camel_tools) (2022.12.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->camel_tools) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->camel_tools) (3.1.0)\n",
            "Building wheels for collected packages: camel-kenlm, docopt, emoji\n",
            "  Building wheel for camel-kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2021.12.27-cp38-cp38-linux_x86_64.whl size=2971715 sha256=4ae18af325ee77bbbfad4143bc4be4660e3d1e73d6f69d1093a41cb1dbcce949\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/7b/f0/837fcdb48cd99564b1163d90392f350cb933fce3bf122eadcd\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=0e79932b324adc82ec3b953bcdcc38248be193ab862821bfad64c5a2cb3e0451\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=184fbf1b9269527d5438eba345c1e31c045f2fa0f1d62fef7e7a2102bba7c215\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n",
            "Successfully built camel-kenlm docopt emoji\n",
            "Installing collected packages: docopt, camel-kenlm, emoji, camel_tools\n",
            "Successfully installed camel-kenlm-2021.12.27 camel_tools-1.4.1 docopt-0.6.2 emoji-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers==4.26.1\n",
        "!git clone --depth 1 --branch v4.26.1 https://github.com/huggingface/transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets\n",
        "!pip3 install evaluate\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!pip install fuzzysearch\n",
        "!pip3 install camel_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ArabicTyDi QA**"
      ],
      "metadata": {
        "id": "nKd9T0Fv_4gA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIu8ZvqJ0_7a"
      },
      "source": [
        "First lets process Arabic TyDi QA dataset. TyDi combine all questions for more than 11 different language so we need to extract the arabic portion and also do some pre-processing. All credits to AUB Mind LAB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oefGTYcdoZpZ",
        "outputId": "c35d2472-dabd-4875-b49e-855500ad664e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 600, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 600 (delta 38), reused 45 (delta 30), pack-reused 535\u001b[K\n",
            "Receiving objects: 100% (600/600), 9.14 MiB | 18.76 MiB/s, done.\n",
            "Resolving deltas: 100% (339/339), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/aub-mind/arabert\n",
        "!cp arabert/examples/question-answering/utils_qa.py .\n",
        "!cp arabert/examples/question-answering/trainer_qa.py .\n",
        "!cp arabert/examples/question-answering/run_qa.py .\n",
        "!cp arabert/examples/question-answering/squad_preprocessing.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqrRzVWNocBh",
        "outputId": "13f988e0-47cc-459e-ef03-0a279cdb9771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-19 16:07:39--  https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-train.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.128, 64.233.189.128, 108.177.97.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58004076 (55M) [application/json]\n",
            "Saving to: ‘tydiqa-goldp-v1.1-train.json’\n",
            "\n",
            "tydiqa-goldp-v1.1-t 100%[===================>]  55.32M  22.3MB/s    in 2.5s    \n",
            "\n",
            "2023-02-19 16:07:42 (22.3 MB/s) - ‘tydiqa-goldp-v1.1-train.json’ saved [58004076/58004076]\n",
            "\n",
            "--2023-02-19 16:07:42--  https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-dev.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.128, 64.233.189.128, 108.177.97.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5617409 (5.4M) [application/json]\n",
            "Saving to: ‘tydiqa-goldp-v1.1-dev.json’\n",
            "\n",
            "tydiqa-goldp-v1.1-d 100%[===================>]   5.36M  6.41MB/s    in 0.8s    \n",
            "\n",
            "2023-02-19 16:07:43 (6.41 MB/s) - ‘tydiqa-goldp-v1.1-dev.json’ saved [5617409/5617409]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-train.json\n",
        "!wget https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-dev.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mGIdA3eJoeRZ"
      },
      "outputs": [],
      "source": [
        "model_name=\"sultan/ArabicT5-Large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a68MU-v3ogL7",
        "outputId": "e8346b2c-008b-4496-b977-320233a055d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-19 16:07:44.064205: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-19 16:07:45.038861: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-19 16:07:45.038946: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-19 16:07:45.038961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "sultan/ArabicT5-Large\n",
            "W0219 16:07:47.724193 140664039696192 preprocess.py:264] Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
            " 38% 18915/49881 [00:00<00:00, 188166.35it/s]WARNING:tensorflow:Could not find answer for question 'arabic-1804271180688859213-0' :\n",
            " 'الإسكندر الثالث المقدوني ، المعروف بأسماء عديدة أخرى أبرزها : الإسكندر الأكبر ، و < b data - parsoid = ' { \" dsr \" : [1849 , 1870 , 3 , 3] } ' > الإسكندر الكبير ، و < b data - parsoid = ' { \" dsr \" : [1873 , 1896 , 3 , 3] } ' > الإسكندر المقدوني ، و < b data - parsoid = ' { \" dsr \" : [1899 , 1924 , 3 , 3] } ' > الإسكندر ذو القرنين ( باليونانية : ؛ نقحرة : ) ، هو أحد ملوك مقدونيا الإغريق ، ومن أشهر القادة العسكريين والفاتحين عبر التاريخ . ولد الإسكندر في مدينة يلا قرابة سنة 356 ق . م ، وتتلمذ على يد الفيلسوف والعالم الشهير أرسطو حتى بلغ ربيعه السادس عشر . وبحلول عامه الثلاثين ، كان قد أسس إحدى أكبر وأعظم الإمبراطوريات التي عرفها العالم القديم ، والتي امتدت من سواحل البحر الأيوني غربا وصولا إلى سلسلة جبال الهيمالايا شرقا . يعد أحد أنجح القادة العسكريين في مسيرتهم ، إذ لم يحصل أن هزم في أي معركة خاضها على الإطلاق . [1]' \n",
            "vs.\n",
            " 'Ἀλέξανδρο'\n",
            "orig answer:\n",
            " 'Ἀλέξανδρο'\n",
            "==================\n",
            "100% 49881/49881 [00:19<00:00, 2535.63it/s]\n",
            "WARNING:tensorflow:Found 0 new answers: \n",
            "WARNING:tensorflow:Found 1 with no answers: \n",
            "WARNING:tensorflow:Found 0 with trunc answers: \n"
          ]
        }
      ],
      "source": [
        "!rm -rf *-pre.json\n",
        "!python squad_preprocessing.py \\\n",
        "  --input_file \"tydiqa-goldp-v1.1-train.json\" \\\n",
        "  --output_file \"tydiqa-goldp-v1.1-train-pre.json\" \\\n",
        "  --model_name=$model_name \\\n",
        "  --filter_tydiqa=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLhP2gAHohvr",
        "outputId": "fb8b398b-906a-4d35-dcd2-15823fe7c835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-19 16:08:14.518312: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-19 16:08:15.495693: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-19 16:08:15.495797: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-19 16:08:15.495809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "sultan/ArabicT5-Large\n",
            "W0219 16:08:17.621287 140142324799296 preprocess.py:264] Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
            "100% 5077/5077 [00:01<00:00, 4995.95it/s]\n",
            "WARNING:tensorflow:Found 0 new answers: \n",
            "WARNING:tensorflow:Found 0 with no answers: \n",
            "WARNING:tensorflow:Found 0 with trunc answers: \n"
          ]
        }
      ],
      "source": [
        "!python squad_preprocessing.py \\\n",
        "  --input_file \"tydiqa-goldp-v1.1-dev.json\" \\\n",
        "  --output_file \"tydiqa-goldp-v1.1-dev-pre.json\" \\\n",
        "  --model_name=$model_name \\\n",
        "  --filter_tydiqa=True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "click on the play button on the code below so the code will saved to this colab as \"artydiqa.py\""
      ],
      "metadata": {
        "id": "_uCD1P-d_h2y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLRe0YISeCpd",
        "outputId": "816afaca-3cd0-436d-c878-713ef72c5265",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "%%writefile artydiqa.py\n",
        "\"\"\"TODO(tydiqa): Add a description here.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "import datasets\n",
        "\n",
        "\n",
        "# TODO(tydiqa): BibTeX citation\n",
        "_CITATION = \"\"\"\\\n",
        "@article{tydiqa,\n",
        "title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},\n",
        "author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}\n",
        "year    = {2020},\n",
        "journal = {Transactions of the Association for Computational Linguistics}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# TODO(tydiqa):\n",
        "_DESCRIPTION = \"\"\"\\\n",
        "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\n",
        "The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\n",
        "expresses -- such that we expect models performing well on this set to generalize across a large number of the languages\n",
        "in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic\n",
        "information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but\n",
        "don’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\n",
        "the use of translation (unlike MLQA and XQuAD).\n",
        "\"\"\"\n",
        "\n",
        "# _URL = \"https://storage.googleapis.com/tydiqa/\"\n",
        "\n",
        "# _PRIMARY_URLS = {\n",
        "#     \"train\": _URL + \"v1.0/tydiqa-v1.0-train.jsonl.gz\",\n",
        "#     \"dev\": _URL + \"v1.0/tydiqa-v1.0-dev.jsonl.gz\",\n",
        "# }\n",
        "# _SECONDARY_URLS = {\n",
        "#     \"train\": _URL + \"v1.1/tydiqa-goldp-v1.1-train.json\",\n",
        "#     \"dev\": _URL + \"v1.1/tydiqa-goldp-v1.1-dev.json\",\n",
        "# }\n",
        "\n",
        "#use this for AraBERTv1 and V2\n",
        "_URL = \"https://storage.googleapis.com/tydiqa/\"\n",
        "_URL2 = \"/content/\"\n",
        "_PRIMARY_URLS = {\n",
        "    \"train\": _URL + \"v1.0/tydiqa-v1.0-train.jsonl.gz\",\n",
        "    \"dev\": _URL + \"v1.0/tydiqa-v1.0-dev.jsonl.gz\",\n",
        "}\n",
        "_SECONDARY_URLS = {\n",
        "    \"train\": _URL2 + \"tydiqa-goldp-v1.1-train-pre.json\",\n",
        "    \"dev\": _URL2 + \"tydiqa-goldp-v1.1-dev-pre.json\",\n",
        "}\n",
        "\n",
        "\n",
        "class TydiqaConfig(datasets.BuilderConfig):\n",
        "\n",
        "    \"\"\" BuilderConfig for Tydiqa\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            **kwargs: keyword arguments forwarded to super.\n",
        "        \"\"\"\n",
        "        super(TydiqaConfig, self).__init__(version=datasets.Version(\"1.0.0\", \"\"), **kwargs)\n",
        "\n",
        "\n",
        "class Tydiqa(datasets.GeneratorBasedBuilder):\n",
        "    \"\"\"TODO(tydiqa): Short description of my dataset.\"\"\"\n",
        "\n",
        "    # TODO(tydiqa): Set up version.\n",
        "    VERSION = datasets.Version(\"0.1.0\")\n",
        "    BUILDER_CONFIGS = [\n",
        "        TydiqaConfig(\n",
        "            name=\"primary_task\",\n",
        "            description=textwrap.dedent(\n",
        "                \"\"\"\\\n",
        "          Passage selection task (SelectP): Given a list of the passages in the article, return either (a) the index of\n",
        "          the passage that answers the question or (b) NULL if no such passage exists.\n",
        "          Minimal answer span task (MinSpan): Given the full text of an article, return one of (a) the start and end\n",
        "          byte indices of the minimal span that completely answers the question; (b) YES or NO if the question requires\n",
        "          a yes/no answer and we can draw a conclusion from the passage; (c) NULL if it is not possible to produce a\n",
        "          minimal answer for this question.\"\"\"\n",
        "            ),\n",
        "        ),\n",
        "        TydiqaConfig(\n",
        "            name=\"secondary_task\",\n",
        "            description=textwrap.dedent(\n",
        "                \"\"\"Gold passage task (GoldP): Given a passage that is guaranteed to contain the\n",
        "          answer, predict the single contiguous span of characters that answers the question. This is more similar to\n",
        "          existing reading comprehension datasets (as opposed to the information-seeking task outlined above).\n",
        "          This task is constructed with two goals in mind: (1) more directly comparing with prior work and (2) providing\n",
        "          a simplified way for researchers to use TyDi QA by providing compatibility with existing code for SQuAD 1.1,\n",
        "          XQuAD, and MLQA. Toward these goals, the gold passage task differs from the primary task in several ways:\n",
        "          only the gold answer passage is provided rather than the entire Wikipedia article;\n",
        "          unanswerable questions have been discarded, similar to MLQA and XQuAD;\n",
        "          we evaluate with the SQuAD 1.1 metrics like XQuAD; and\n",
        "         Thai and Japanese are removed since the lack of whitespace breaks some tools.\n",
        "          \"\"\"\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    def _info(self):\n",
        "        # TODO(tydiqa): Specifies the datasets.DatasetInfo object\n",
        "        if self.config.name == \"primary_task\":\n",
        "            return datasets.DatasetInfo(\n",
        "                # This is the description that will appear on the datasets page.\n",
        "                description=_DESCRIPTION,\n",
        "                # datasets.features.FeatureConnectors\n",
        "                features=datasets.Features(\n",
        "                    {\n",
        "                        \"passage_answer_candidates\": datasets.features.Sequence(\n",
        "                            {\n",
        "                                \"plaintext_start_byte\": datasets.Value(\"int32\"),\n",
        "                                \"plaintext_end_byte\": datasets.Value(\"int32\"),\n",
        "                            }\n",
        "                        ),\n",
        "                        \"question_text\": datasets.Value(\"string\"),\n",
        "                        \"document_title\": datasets.Value(\"string\"),\n",
        "                        \"language\": datasets.Value(\"string\"),\n",
        "                        \"annotations\": datasets.features.Sequence(\n",
        "                            {\n",
        "                                # 'annotation_id': datasets.Value('variant'),\n",
        "                                \"passage_answer_candidate_index\": datasets.Value(\"int32\"),\n",
        "                                \"minimal_answers_start_byte\": datasets.Value(\"int32\"),\n",
        "                                \"minimal_answers_end_byte\": datasets.Value(\"int32\"),\n",
        "                                \"yes_no_answer\": datasets.Value(\"string\"),\n",
        "                            }\n",
        "                        ),\n",
        "                        \"document_plaintext\": datasets.Value(\"string\"),\n",
        "                        # 'example_id': datasets.Value('variant'),\n",
        "                        \"document_url\": datasets.Value(\"string\")\n",
        "                        # These are the features of your dataset like images, labels ...\n",
        "                    }\n",
        "                ),\n",
        "                # If there's a common (input, target) tuple from the features,\n",
        "                # specify them here. They'll be used if as_supervised=True in\n",
        "                # builder.as_dataset.\n",
        "                supervised_keys=None,\n",
        "                # Homepage of the dataset for documentation\n",
        "                homepage=\"https://github.com/google-research-datasets/tydiqa\",\n",
        "                citation=_CITATION,\n",
        "            )\n",
        "        elif self.config.name == \"secondary_task\":\n",
        "            return datasets.DatasetInfo(\n",
        "                description=_DESCRIPTION,\n",
        "                features=datasets.Features(\n",
        "                    {\n",
        "                        \"id\": datasets.Value(\"string\"),\n",
        "                        \"title\": datasets.Value(\"string\"),\n",
        "                        \"context\": datasets.Value(\"string\"),\n",
        "                        \"question\": datasets.Value(\"string\"),\n",
        "                        \"answers\": datasets.features.Sequence(\n",
        "                            {\n",
        "                                \"text\": datasets.Value(\"string\"),\n",
        "                                \"answer_start\": datasets.Value(\"int32\"),\n",
        "                            }\n",
        "                        ),\n",
        "                    }\n",
        "                ),\n",
        "                # No default supervised_keys (as we have to pass both question\n",
        "                # and context as input).\n",
        "                supervised_keys=None,\n",
        "                homepage=\"https://github.com/google-research-datasets/tydiqa\",\n",
        "                citation=_CITATION,\n",
        "            )\n",
        "\n",
        "    def _split_generators(self, dl_manager):\n",
        "        \"\"\"Returns SplitGenerators.\"\"\"\n",
        "        # TODO(tydiqa): Downloads the data and defines the splits\n",
        "        # dl_manager is a datasets.download.DownloadManager that can be used to\n",
        "        # download and extract URLs\n",
        "        primary_downloaded = dl_manager.download_and_extract(_PRIMARY_URLS)\n",
        "        secondary_downloaded = dl_manager.download_and_extract(_SECONDARY_URLS)\n",
        "        if self.config.name == \"primary_task\":\n",
        "            return [\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.TRAIN,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": primary_downloaded[\"train\"]},\n",
        "                ),\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.VALIDATION,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": primary_downloaded[\"dev\"]},\n",
        "                ),\n",
        "            ]\n",
        "        elif self.config.name == \"secondary_task\":\n",
        "            return [\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.TRAIN,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": secondary_downloaded[\"train\"]},\n",
        "                ),\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.VALIDATION,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": secondary_downloaded[\"dev\"]},\n",
        "                ),\n",
        "            ]\n",
        "\n",
        "    def _generate_examples(self, filepath):\n",
        "        \"\"\"Yields examples.\"\"\"\n",
        "        # TODO(tydiqa): Yields (key, example) tuples from the dataset\n",
        "        if self.config.name == \"primary_task\":\n",
        "            with open(filepath, encoding=\"utf-8\") as f:\n",
        "                for id_, row in enumerate(f):\n",
        "                    data = json.loads(row)\n",
        "                    passages = data[\"passage_answer_candidates\"]\n",
        "                    end_byte = [passage[\"plaintext_end_byte\"] for passage in passages]\n",
        "                    start_byte = [passage[\"plaintext_start_byte\"] for passage in passages]\n",
        "                    title = data[\"document_title\"]\n",
        "                    lang = data[\"language\"]\n",
        "                    question = data[\"question_text\"]\n",
        "                    annotations = data[\"annotations\"]\n",
        "                    # annot_ids = [annotation[\"annotation_id\"] for annotation in annotations]\n",
        "                    yes_no_answers = [annotation[\"yes_no_answer\"] for annotation in annotations]\n",
        "                    min_answers_end_byte = [\n",
        "                        annotation[\"minimal_answer\"][\"plaintext_end_byte\"] for annotation in annotations\n",
        "                    ]\n",
        "                    min_answers_start_byte = [\n",
        "                        annotation[\"minimal_answer\"][\"plaintext_start_byte\"] for annotation in annotations\n",
        "                    ]\n",
        "                    passage_cand_answers = [\n",
        "                        annotation[\"passage_answer\"][\"candidate_index\"] for annotation in annotations\n",
        "                    ]\n",
        "                    doc = data[\"document_plaintext\"]\n",
        "                    # example_id = data[\"example_id\"]\n",
        "                    url = data[\"document_url\"]\n",
        "                    yield id_, {\n",
        "                        \"passage_answer_candidates\": {\n",
        "                            \"plaintext_start_byte\": start_byte,\n",
        "                            \"plaintext_end_byte\": end_byte,\n",
        "                        },\n",
        "                        \"question_text\": question,\n",
        "                        \"document_title\": title,\n",
        "                        \"language\": lang,\n",
        "                        \"annotations\": {\n",
        "                            # 'annotation_id': annot_ids,\n",
        "                            \"passage_answer_candidate_index\": passage_cand_answers,\n",
        "                            \"minimal_answers_start_byte\": min_answers_start_byte,\n",
        "                            \"minimal_answers_end_byte\": min_answers_end_byte,\n",
        "                            \"yes_no_answer\": yes_no_answers,\n",
        "                        },\n",
        "                        \"document_plaintext\": doc,\n",
        "                        # 'example_id': example_id,\n",
        "                        \"document_url\": url,\n",
        "                    }\n",
        "        elif self.config.name == \"secondary_task\":\n",
        "            with open(filepath, encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "                for article in data[\"data\"]:\n",
        "                    title = article.get(\"title\", \"\").strip()\n",
        "                    for paragraph in article[\"paragraphs\"]:\n",
        "                        context = paragraph[\"context\"].strip()\n",
        "                        for qa in paragraph[\"qas\"]:\n",
        "                            question = qa[\"question\"].strip()\n",
        "                            id_ = qa[\"id\"]\n",
        "                            if \"arabic\" not in id_:\n",
        "                              continue\n",
        "                            answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"] if answer[\"answer_start\"] != -1]                            \n",
        "                            answers = [answer[\"text\"].strip() for answer in qa[\"answers\"] if answer[\"answer_start\"] != -1]\n",
        "                            if len(answers) == 0 or len(answer_starts)==0:\n",
        "                              print(\"question skipped\")\n",
        "                              continue\n",
        "                            assert len(answer_starts)==len(answers)\n",
        "                            # Features currently used are \"context\", \"question\", and \"answers\".\n",
        "                            # Others are extracted here for the ease of future expansions.\n",
        "                            yield id_, {\n",
        "                                \"title\": title,\n",
        "                                \"context\": context,\n",
        "                                \"question\": question,\n",
        "                                \"id\": id_,\n",
        "                                \"answers\": {\n",
        "                                    \"answer_start\": answer_starts,\n",
        "                                    \"text\": answers,\n",
        "                                },\n",
        "                            }"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing artydiqa.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaVWsUF8IP15",
        "outputId": "f33efe58-274f-4100-f552-a1291dfbc58e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-19 17:02:55.047535: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-19 17:02:55.938577: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-19 17:02:55.938696: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-19 17:02:55.938717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/runs/Feb19_17-02-58_c015a1dc4b85,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=4.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=out,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out,\n",
            "save_on_each_node=False,\n",
            "save_steps=500000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=500,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/artydiqa/294ddd237cc71219f6df7e436ee824812060085d5593f433627715c4379c01f3\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/artydiqa/secondary_task/1.0.0/294ddd237cc71219f6df7e436ee824812060085d5593f433627715c4379c01f3\n",
            "WARNING:datasets.builder:Found cached dataset artydiqa (/root/.cache/huggingface/datasets/artydiqa/secondary_task/1.0.0/294ddd237cc71219f6df7e436ee824812060085d5593f433627715c4379c01f3)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/artydiqa/secondary_task/1.0.0/294ddd237cc71219f6df7e436ee824812060085d5593f433627715c4379c01f3\n",
            "100% 2/2 [00:00<00:00, 621.01it/s]\n",
            "[INFO|configuration_utils.py:660] 2023-02-19 17:03:00,941 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicTransformer-intermediate/snapshots/49171ae02f8ed9d04e2e5575637b9118471bef43/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-19 17:03:00,942 >> Model config FunnelConfig {\n",
            "  \"_name_or_path\": \"sultan/ArabicTransformer-intermediate\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"architectures\": [\n",
            "    \"FunnelModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_type\": \"relative_shift\",\n",
            "  \"block_repeats\": [\n",
            "    1,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"block_sizes\": [\n",
            "    6,\n",
            "    6,\n",
            "    6\n",
            "  ],\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"initializer_range\": 0.1,\n",
            "  \"initializer_std\": null,\n",
            "  \"layer_norm_eps\": 1e-09,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"funnel\",\n",
            "  \"n_head\": 12,\n",
            "  \"num_decoder_layers\": 2,\n",
            "  \"pool_q_only\": true,\n",
            "  \"pooling_type\": \"mean\",\n",
            "  \"rel_attn_type\": \"factorized\",\n",
            "  \"separate_cls\": true,\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"truncate_seq\": true,\n",
            "  \"type_vocab_size\": 3,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:458] 2023-02-19 17:03:01,697 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:660] 2023-02-19 17:03:02,462 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicTransformer-intermediate/snapshots/49171ae02f8ed9d04e2e5575637b9118471bef43/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-19 17:03:02,463 >> Model config FunnelConfig {\n",
            "  \"_name_or_path\": \"sultan/ArabicTransformer-intermediate\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"architectures\": [\n",
            "    \"FunnelModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_type\": \"relative_shift\",\n",
            "  \"block_repeats\": [\n",
            "    1,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"block_sizes\": [\n",
            "    6,\n",
            "    6,\n",
            "    6\n",
            "  ],\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"initializer_range\": 0.1,\n",
            "  \"initializer_std\": null,\n",
            "  \"layer_norm_eps\": 1e-09,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"funnel\",\n",
            "  \"n_head\": 12,\n",
            "  \"num_decoder_layers\": 2,\n",
            "  \"pool_q_only\": true,\n",
            "  \"pooling_type\": \"mean\",\n",
            "  \"rel_attn_type\": \"factorized\",\n",
            "  \"separate_cls\": true,\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"truncate_seq\": true,\n",
            "  \"type_vocab_size\": 3,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-19 17:03:03,989 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sultan--ArabicTransformer-intermediate/snapshots/49171ae02f8ed9d04e2e5575637b9118471bef43/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-19 17:03:03,989 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-19 17:03:03,989 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-19 17:03:03,989 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-19 17:03:03,989 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:660] 2023-02-19 17:03:03,989 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicTransformer-intermediate/snapshots/49171ae02f8ed9d04e2e5575637b9118471bef43/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-19 17:03:03,990 >> Model config FunnelConfig {\n",
            "  \"_name_or_path\": \"sultan/ArabicTransformer-intermediate\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"architectures\": [\n",
            "    \"FunnelModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_type\": \"relative_shift\",\n",
            "  \"block_repeats\": [\n",
            "    1,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"block_sizes\": [\n",
            "    6,\n",
            "    6,\n",
            "    6\n",
            "  ],\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"initializer_range\": 0.1,\n",
            "  \"initializer_std\": null,\n",
            "  \"layer_norm_eps\": 1e-09,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"funnel\",\n",
            "  \"n_head\": 12,\n",
            "  \"num_decoder_layers\": 2,\n",
            "  \"pool_q_only\": true,\n",
            "  \"pooling_type\": \"mean\",\n",
            "  \"rel_attn_type\": \"factorized\",\n",
            "  \"separate_cls\": true,\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"truncate_seq\": true,\n",
            "  \"type_vocab_size\": 3,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils.py:426] 2023-02-19 17:03:04,040 >> Adding <s> to the vocabulary\n",
            "[INFO|tokenization_utils.py:426] 2023-02-19 17:03:04,040 >> Adding </s> to the vocabulary\n",
            "[WARNING|logging.py:281] 2023-02-19 17:03:04,040 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:660] 2023-02-19 17:03:04,041 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--ArabicTransformer-intermediate/snapshots/49171ae02f8ed9d04e2e5575637b9118471bef43/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-19 17:03:04,041 >> Model config FunnelConfig {\n",
            "  \"_name_or_path\": \"sultan/ArabicTransformer-intermediate\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"architectures\": [\n",
            "    \"FunnelModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_type\": \"relative_shift\",\n",
            "  \"block_repeats\": [\n",
            "    1,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"block_sizes\": [\n",
            "    6,\n",
            "    6,\n",
            "    6\n",
            "  ],\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"initializer_range\": 0.1,\n",
            "  \"initializer_std\": null,\n",
            "  \"layer_norm_eps\": 1e-09,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"funnel\",\n",
            "  \"n_head\": 12,\n",
            "  \"num_decoder_layers\": 2,\n",
            "  \"pool_q_only\": true,\n",
            "  \"pooling_type\": \"mean\",\n",
            "  \"rel_attn_type\": \"factorized\",\n",
            "  \"separate_cls\": true,\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"truncate_seq\": true,\n",
            "  \"type_vocab_size\": 3,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:281] 2023-02-19 17:03:04,062 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|modeling_utils.py:2275] 2023-02-19 17:03:04,087 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sultan--ArabicTransformer-intermediate/snapshots/49171ae02f8ed9d04e2e5575637b9118471bef43/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2857] 2023-02-19 17:03:06,397 >> All model checkpoint weights were used when initializing FunnelForQuestionAnswering.\n",
            "\n",
            "[WARNING|modeling_utils.py:2859] 2023-02-19 17:03:06,397 >> Some weights of FunnelForQuestionAnswering were not initialized from the model checkpoint at sultan/ArabicTransformer-intermediate and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/15 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/artydiqa/secondary_task/1.0.0/294ddd237cc71219f6df7e436ee824812060085d5593f433627715c4379c01f3/cache-f395ddf536d338f3.arrow\n",
            "Running tokenizer on train dataset: 100% 15/15 [00:07<00:00,  2.05ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/artydiqa/secondary_task/1.0.0/294ddd237cc71219f6df7e436ee824812060085d5593f433627715c4379c01f3/cache-fc4ee34bf5ab9c77.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  2.01ba/s]\n",
            "[INFO|trainer.py:565] 2023-02-19 17:03:18,387 >> Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1650] 2023-02-19 17:03:18,396 >> ***** Running training *****\n",
            "[INFO|trainer.py:1651] 2023-02-19 17:03:18,396 >>   Num examples = 15381\n",
            "[INFO|trainer.py:1652] 2023-02-19 17:03:18,396 >>   Num Epochs = 4\n",
            "[INFO|trainer.py:1653] 2023-02-19 17:03:18,396 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1654] 2023-02-19 17:03:18,396 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1655] 2023-02-19 17:03:18,396 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1656] 2023-02-19 17:03:18,396 >>   Total optimization steps = 3848\n",
            "[INFO|trainer.py:1657] 2023-02-19 17:03:18,397 >>   Number of trainable parameters = 192018434\n",
            "{'loss': 2.8749, 'learning_rate': 2.976e-05, 'epoch': 0.52}\n",
            " 25% 962/3848 [12:29<30:55,  1.56it/s][INFO|trainer.py:710] 2023-02-19 17:15:47,815 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 17:15:47,817 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 17:15:47,817 >>   Num examples = 944\n",
            "[INFO|trainer.py:2969] 2023-02-19 17:15:47,817 >>   Batch size = 8\n",
            "\n",
            "  0% 0/118 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/118 [00:00<00:07, 14.72it/s]\u001b[A\n",
            "  3% 4/118 [00:00<00:12,  9.26it/s]\u001b[A\n",
            "  5% 6/118 [00:00<00:13,  8.24it/s]\u001b[A\n",
            "  6% 7/118 [00:00<00:13,  7.98it/s]\u001b[A\n",
            "  7% 8/118 [00:00<00:14,  7.79it/s]\u001b[A\n",
            "  8% 9/118 [00:01<00:14,  7.62it/s]\u001b[A\n",
            "  8% 10/118 [00:01<00:14,  7.52it/s]\u001b[A\n",
            "  9% 11/118 [00:01<00:14,  7.47it/s]\u001b[A\n",
            " 10% 12/118 [00:01<00:14,  7.43it/s]\u001b[A\n",
            " 11% 13/118 [00:01<00:14,  7.39it/s]\u001b[A\n",
            " 12% 14/118 [00:01<00:14,  7.34it/s]\u001b[A\n",
            " 13% 15/118 [00:01<00:14,  7.33it/s]\u001b[A\n",
            " 14% 16/118 [00:02<00:13,  7.32it/s]\u001b[A\n",
            " 14% 17/118 [00:02<00:13,  7.33it/s]\u001b[A\n",
            " 15% 18/118 [00:02<00:13,  7.31it/s]\u001b[A\n",
            " 16% 19/118 [00:02<00:13,  7.31it/s]\u001b[A\n",
            " 17% 20/118 [00:02<00:13,  7.31it/s]\u001b[A\n",
            " 18% 21/118 [00:02<00:13,  7.31it/s]\u001b[A\n",
            " 19% 22/118 [00:02<00:13,  7.29it/s]\u001b[A\n",
            " 19% 23/118 [00:03<00:12,  7.31it/s]\u001b[A\n",
            " 20% 24/118 [00:03<00:12,  7.33it/s]\u001b[A\n",
            " 21% 25/118 [00:03<00:12,  7.32it/s]\u001b[A\n",
            " 22% 26/118 [00:03<00:12,  7.29it/s]\u001b[A\n",
            " 23% 27/118 [00:03<00:12,  7.30it/s]\u001b[A\n",
            " 24% 28/118 [00:03<00:12,  7.30it/s]\u001b[A\n",
            " 25% 29/118 [00:03<00:12,  7.30it/s]\u001b[A\n",
            " 25% 30/118 [00:03<00:12,  7.29it/s]\u001b[A\n",
            " 26% 31/118 [00:04<00:11,  7.29it/s]\u001b[A\n",
            " 27% 32/118 [00:04<00:11,  7.32it/s]\u001b[A\n",
            " 28% 33/118 [00:04<00:11,  7.31it/s]\u001b[A\n",
            " 29% 34/118 [00:04<00:11,  7.29it/s]\u001b[A\n",
            " 30% 35/118 [00:04<00:11,  7.29it/s]\u001b[A\n",
            " 31% 36/118 [00:04<00:11,  7.30it/s]\u001b[A\n",
            " 31% 37/118 [00:04<00:11,  7.29it/s]\u001b[A\n",
            " 32% 38/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 33% 39/118 [00:05<00:10,  7.27it/s]\u001b[A\n",
            " 34% 40/118 [00:05<00:10,  7.29it/s]\u001b[A\n",
            " 35% 41/118 [00:05<00:10,  7.30it/s]\u001b[A\n",
            " 36% 42/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 36% 43/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 37% 44/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 38% 45/118 [00:06<00:10,  7.29it/s]\u001b[A\n",
            " 39% 46/118 [00:06<00:09,  7.27it/s]\u001b[A\n",
            " 40% 47/118 [00:06<00:09,  7.27it/s]\u001b[A\n",
            " 41% 48/118 [00:06<00:09,  7.30it/s]\u001b[A\n",
            " 42% 49/118 [00:06<00:09,  7.30it/s]\u001b[A\n",
            " 42% 50/118 [00:06<00:09,  7.28it/s]\u001b[A\n",
            " 43% 51/118 [00:06<00:09,  7.28it/s]\u001b[A\n",
            " 44% 52/118 [00:06<00:09,  7.28it/s]\u001b[A\n",
            " 45% 53/118 [00:07<00:08,  7.26it/s]\u001b[A\n",
            " 46% 54/118 [00:07<00:08,  7.25it/s]\u001b[A\n",
            " 47% 55/118 [00:07<00:08,  7.23it/s]\u001b[A\n",
            " 47% 56/118 [00:07<00:08,  7.26it/s]\u001b[A\n",
            " 48% 57/118 [00:07<00:08,  7.27it/s]\u001b[A\n",
            " 49% 58/118 [00:07<00:08,  7.26it/s]\u001b[A\n",
            " 50% 59/118 [00:07<00:08,  7.27it/s]\u001b[A\n",
            " 51% 60/118 [00:08<00:07,  7.26it/s]\u001b[A\n",
            " 52% 61/118 [00:08<00:07,  7.25it/s]\u001b[A\n",
            " 53% 62/118 [00:08<00:07,  7.25it/s]\u001b[A\n",
            " 53% 63/118 [00:08<00:07,  7.25it/s]\u001b[A\n",
            " 54% 64/118 [00:08<00:07,  7.25it/s]\u001b[A\n",
            " 55% 65/118 [00:08<00:07,  7.27it/s]\u001b[A\n",
            " 56% 66/118 [00:08<00:07,  7.26it/s]\u001b[A\n",
            " 57% 67/118 [00:09<00:07,  7.28it/s]\u001b[A\n",
            " 58% 68/118 [00:09<00:06,  7.30it/s]\u001b[A\n",
            " 58% 69/118 [00:09<00:06,  7.29it/s]\u001b[A\n",
            " 59% 70/118 [00:09<00:06,  7.28it/s]\u001b[A\n",
            " 60% 71/118 [00:09<00:06,  7.29it/s]\u001b[A\n",
            " 61% 72/118 [00:09<00:06,  7.30it/s]\u001b[A\n",
            " 62% 73/118 [00:09<00:06,  7.30it/s]\u001b[A\n",
            " 63% 74/118 [00:10<00:06,  7.30it/s]\u001b[A\n",
            " 64% 75/118 [00:10<00:05,  7.31it/s]\u001b[A\n",
            " 64% 76/118 [00:10<00:05,  7.32it/s]\u001b[A\n",
            " 65% 77/118 [00:10<00:05,  7.31it/s]\u001b[A\n",
            " 66% 78/118 [00:10<00:05,  7.30it/s]\u001b[A\n",
            " 67% 79/118 [00:10<00:05,  7.31it/s]\u001b[A\n",
            " 68% 80/118 [00:10<00:05,  7.32it/s]\u001b[A\n",
            " 69% 81/118 [00:10<00:05,  7.32it/s]\u001b[A\n",
            " 69% 82/118 [00:11<00:04,  7.32it/s]\u001b[A\n",
            " 70% 83/118 [00:11<00:04,  7.32it/s]\u001b[A\n",
            " 71% 84/118 [00:11<00:04,  7.32it/s]\u001b[A\n",
            " 72% 85/118 [00:11<00:04,  7.28it/s]\u001b[A\n",
            " 73% 86/118 [00:11<00:04,  7.26it/s]\u001b[A\n",
            " 74% 87/118 [00:11<00:04,  7.26it/s]\u001b[A\n",
            " 75% 88/118 [00:11<00:04,  7.28it/s]\u001b[A\n",
            " 75% 89/118 [00:12<00:03,  7.28it/s]\u001b[A\n",
            " 76% 90/118 [00:12<00:03,  7.29it/s]\u001b[A\n",
            " 77% 91/118 [00:12<00:03,  7.28it/s]\u001b[A\n",
            " 78% 92/118 [00:12<00:03,  7.30it/s]\u001b[A\n",
            " 79% 93/118 [00:12<00:03,  7.30it/s]\u001b[A\n",
            " 80% 94/118 [00:12<00:03,  7.28it/s]\u001b[A\n",
            " 81% 95/118 [00:12<00:03,  7.28it/s]\u001b[A\n",
            " 81% 96/118 [00:13<00:03,  7.27it/s]\u001b[A\n",
            " 82% 97/118 [00:13<00:02,  7.27it/s]\u001b[A\n",
            " 83% 98/118 [00:13<00:02,  7.27it/s]\u001b[A\n",
            " 84% 99/118 [00:13<00:02,  7.28it/s]\u001b[A\n",
            " 85% 100/118 [00:13<00:02,  7.27it/s]\u001b[A\n",
            " 86% 101/118 [00:13<00:02,  7.28it/s]\u001b[A\n",
            " 86% 102/118 [00:13<00:02,  7.29it/s]\u001b[A\n",
            " 87% 103/118 [00:13<00:02,  7.29it/s]\u001b[A\n",
            " 88% 104/118 [00:14<00:01,  7.29it/s]\u001b[A\n",
            " 89% 105/118 [00:14<00:01,  7.27it/s]\u001b[A\n",
            " 90% 106/118 [00:14<00:01,  7.27it/s]\u001b[A\n",
            " 91% 107/118 [00:14<00:01,  7.28it/s]\u001b[A\n",
            " 92% 108/118 [00:14<00:01,  7.27it/s]\u001b[A\n",
            " 92% 109/118 [00:14<00:01,  7.27it/s]\u001b[A\n",
            " 93% 110/118 [00:14<00:01,  7.28it/s]\u001b[A\n",
            " 94% 111/118 [00:15<00:00,  7.30it/s]\u001b[A\n",
            " 95% 112/118 [00:15<00:00,  7.29it/s]\u001b[A\n",
            " 96% 113/118 [00:15<00:00,  7.29it/s]\u001b[A\n",
            " 97% 114/118 [00:15<00:00,  7.31it/s]\u001b[A\n",
            " 97% 115/118 [00:15<00:00,  7.31it/s]\u001b[A\n",
            " 98% 116/118 [00:15<00:00,  7.30it/s]\u001b[A\n",
            " 99% 117/118 [00:15<00:00,  7.30it/s]\u001b[A\n",
            "100% 118/118 [00:16<00:00,  7.27it/s]\u001b[AINFO:utils_qa:Post-processing 921 example predictions split into 944 features.\n",
            "\n",
            "\n",
            "  0% 0/921 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4% 34/921 [00:00<00:02, 333.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "  8% 70/921 [00:00<00:02, 347.34it/s]\u001b[A\u001b[A\n",
            "\n",
            " 11% 105/921 [00:00<00:02, 344.89it/s]\u001b[A\u001b[A\n",
            "\n",
            " 15% 140/921 [00:00<00:02, 340.66it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19% 176/921 [00:00<00:02, 346.75it/s]\u001b[A\u001b[A\n",
            "\n",
            " 23% 211/921 [00:00<00:02, 347.15it/s]\u001b[A\u001b[A\n",
            "\n",
            " 27% 248/921 [00:00<00:01, 353.51it/s]\u001b[A\u001b[A\n",
            "\n",
            " 31% 285/921 [00:00<00:01, 355.84it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35% 321/921 [00:00<00:01, 355.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 39% 358/921 [00:01<00:01, 357.22it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43% 395/921 [00:01<00:01, 358.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 47% 432/921 [00:01<00:01, 359.92it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51% 468/921 [00:01<00:01, 352.29it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55% 504/921 [00:01<00:01, 354.27it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59% 540/921 [00:01<00:01, 355.50it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63% 576/921 [00:01<00:01, 341.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 66% 611/921 [00:01<00:00, 336.91it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70% 645/921 [00:01<00:00, 333.48it/s]\u001b[A\u001b[A\n",
            "\n",
            " 74% 679/921 [00:01<00:00, 313.51it/s]\u001b[A\u001b[A\n",
            "\n",
            " 77% 712/921 [00:02<00:00, 316.74it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81% 746/921 [00:02<00:00, 320.87it/s]\u001b[A\u001b[A\n",
            "\n",
            " 85% 781/921 [00:02<00:00, 328.67it/s]\u001b[A\u001b[A\n",
            "\n",
            " 88% 815/921 [00:02<00:00, 331.62it/s]\u001b[A\u001b[A\n",
            "\n",
            " 92% 849/921 [00:02<00:00, 329.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 96% 882/921 [00:02<00:00, 327.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 921/921 [00:02<00:00, 338.88it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                      \n",
            "\u001b[A{'eval_exact_match': 69.27252985884908, 'eval_f1': 83.09060274511431, 'eval_runtime': 16.201, 'eval_samples_per_second': 58.268, 'eval_steps_per_second': 7.284, 'epoch': 1.0}\n",
            " 25% 962/3848 [12:49<30:55,  1.56it/s]\n",
            "100% 118/118 [00:20<00:00,  7.27it/s]\u001b[A\n",
            "{'loss': 1.1464, 'learning_rate': 2.556451612903226e-05, 'epoch': 1.04}\n",
            "{'loss': 0.8752, 'learning_rate': 2.1084229390681004e-05, 'epoch': 1.56}\n",
            " 50% 1924/3848 [25:17<20:35,  1.56it/s][INFO|trainer.py:710] 2023-02-19 17:28:35,880 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 17:28:35,882 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 17:28:35,882 >>   Num examples = 944\n",
            "[INFO|trainer.py:2969] 2023-02-19 17:28:35,882 >>   Batch size = 8\n",
            "\n",
            "  0% 0/118 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/118 [00:00<00:07, 14.58it/s]\u001b[A\n",
            "  3% 4/118 [00:00<00:12,  9.11it/s]\u001b[A\n",
            "  5% 6/118 [00:00<00:13,  8.16it/s]\u001b[A\n",
            "  6% 7/118 [00:00<00:14,  7.90it/s]\u001b[A\n",
            "  7% 8/118 [00:00<00:14,  7.72it/s]\u001b[A\n",
            "  8% 9/118 [00:01<00:14,  7.54it/s]\u001b[A\n",
            "  8% 10/118 [00:01<00:14,  7.47it/s]\u001b[A\n",
            "  9% 11/118 [00:01<00:14,  7.39it/s]\u001b[A\n",
            " 10% 12/118 [00:01<00:14,  7.35it/s]\u001b[A\n",
            " 11% 13/118 [00:01<00:14,  7.28it/s]\u001b[A\n",
            " 12% 14/118 [00:01<00:14,  7.27it/s]\u001b[A\n",
            " 13% 15/118 [00:01<00:14,  7.28it/s]\u001b[A\n",
            " 14% 16/118 [00:02<00:14,  7.27it/s]\u001b[A\n",
            " 14% 17/118 [00:02<00:13,  7.25it/s]\u001b[A\n",
            " 15% 18/118 [00:02<00:13,  7.26it/s]\u001b[A\n",
            " 16% 19/118 [00:02<00:13,  7.26it/s]\u001b[A\n",
            " 17% 20/118 [00:02<00:13,  7.26it/s]\u001b[A\n",
            " 18% 21/118 [00:02<00:13,  7.25it/s]\u001b[A\n",
            " 19% 22/118 [00:02<00:13,  7.26it/s]\u001b[A\n",
            " 19% 23/118 [00:03<00:13,  7.24it/s]\u001b[A\n",
            " 20% 24/118 [00:03<00:12,  7.23it/s]\u001b[A\n",
            " 21% 25/118 [00:03<00:12,  7.22it/s]\u001b[A\n",
            " 22% 26/118 [00:03<00:12,  7.24it/s]\u001b[A\n",
            " 23% 27/118 [00:03<00:12,  7.24it/s]\u001b[A\n",
            " 24% 28/118 [00:03<00:12,  7.25it/s]\u001b[A\n",
            " 25% 29/118 [00:03<00:12,  7.24it/s]\u001b[A\n",
            " 25% 30/118 [00:04<00:12,  7.22it/s]\u001b[A\n",
            " 26% 31/118 [00:04<00:12,  7.22it/s]\u001b[A\n",
            " 27% 32/118 [00:04<00:11,  7.21it/s]\u001b[A\n",
            " 28% 33/118 [00:04<00:11,  7.23it/s]\u001b[A\n",
            " 29% 34/118 [00:04<00:11,  7.26it/s]\u001b[A\n",
            " 30% 35/118 [00:04<00:11,  7.27it/s]\u001b[A\n",
            " 31% 36/118 [00:04<00:11,  7.26it/s]\u001b[A\n",
            " 31% 37/118 [00:04<00:11,  7.27it/s]\u001b[A\n",
            " 32% 38/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 33% 39/118 [00:05<00:10,  7.29it/s]\u001b[A\n",
            " 34% 40/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 35% 41/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 36% 42/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 36% 43/118 [00:05<00:10,  7.30it/s]\u001b[A\n",
            " 37% 44/118 [00:05<00:10,  7.29it/s]\u001b[A\n",
            " 38% 45/118 [00:06<00:10,  7.28it/s]\u001b[A\n",
            " 39% 46/118 [00:06<00:09,  7.29it/s]\u001b[A\n",
            " 40% 47/118 [00:06<00:09,  7.31it/s]\u001b[A\n",
            " 41% 48/118 [00:06<00:09,  7.31it/s]\u001b[A\n",
            " 42% 49/118 [00:06<00:09,  7.29it/s]\u001b[A\n",
            " 42% 50/118 [00:06<00:09,  7.27it/s]\u001b[A\n",
            " 43% 51/118 [00:06<00:09,  7.29it/s]\u001b[A\n",
            " 44% 52/118 [00:07<00:09,  7.27it/s]\u001b[A\n",
            " 45% 53/118 [00:07<00:08,  7.24it/s]\u001b[A\n",
            " 46% 54/118 [00:07<00:08,  7.24it/s]\u001b[A\n",
            " 47% 55/118 [00:07<00:08,  7.25it/s]\u001b[A\n",
            " 47% 56/118 [00:07<00:08,  7.27it/s]\u001b[A\n",
            " 48% 57/118 [00:07<00:08,  7.27it/s]\u001b[A\n",
            " 49% 58/118 [00:07<00:08,  7.26it/s]\u001b[A\n",
            " 50% 59/118 [00:07<00:08,  7.27it/s]\u001b[A\n",
            " 51% 60/118 [00:08<00:07,  7.29it/s]\u001b[A\n",
            " 52% 61/118 [00:08<00:07,  7.29it/s]\u001b[A\n",
            " 53% 62/118 [00:08<00:07,  7.25it/s]\u001b[A\n",
            " 53% 63/118 [00:08<00:07,  7.28it/s]\u001b[A\n",
            " 54% 64/118 [00:08<00:07,  7.30it/s]\u001b[A\n",
            " 55% 65/118 [00:08<00:07,  7.30it/s]\u001b[A\n",
            " 56% 66/118 [00:08<00:07,  7.29it/s]\u001b[A\n",
            " 57% 67/118 [00:09<00:07,  7.29it/s]\u001b[A\n",
            " 58% 68/118 [00:09<00:06,  7.28it/s]\u001b[A\n",
            " 58% 69/118 [00:09<00:06,  7.28it/s]\u001b[A\n",
            " 59% 70/118 [00:09<00:06,  7.26it/s]\u001b[A\n",
            " 60% 71/118 [00:09<00:06,  7.26it/s]\u001b[A\n",
            " 61% 72/118 [00:09<00:06,  7.26it/s]\u001b[A\n",
            " 62% 73/118 [00:09<00:06,  7.26it/s]\u001b[A\n",
            " 63% 74/118 [00:10<00:06,  7.26it/s]\u001b[A\n",
            " 64% 75/118 [00:10<00:05,  7.26it/s]\u001b[A\n",
            " 64% 76/118 [00:10<00:05,  7.26it/s]\u001b[A\n",
            " 65% 77/118 [00:10<00:05,  7.27it/s]\u001b[A\n",
            " 66% 78/118 [00:10<00:05,  7.26it/s]\u001b[A\n",
            " 67% 79/118 [00:10<00:05,  7.26it/s]\u001b[A\n",
            " 68% 80/118 [00:10<00:05,  7.26it/s]\u001b[A\n",
            " 69% 81/118 [00:11<00:05,  7.27it/s]\u001b[A\n",
            " 69% 82/118 [00:11<00:04,  7.28it/s]\u001b[A\n",
            " 70% 83/118 [00:11<00:04,  7.28it/s]\u001b[A\n",
            " 71% 84/118 [00:11<00:04,  7.28it/s]\u001b[A\n",
            " 72% 85/118 [00:11<00:04,  7.27it/s]\u001b[A\n",
            " 73% 86/118 [00:11<00:04,  7.29it/s]\u001b[A\n",
            " 74% 87/118 [00:11<00:04,  7.27it/s]\u001b[A\n",
            " 75% 88/118 [00:11<00:04,  7.26it/s]\u001b[A\n",
            " 75% 89/118 [00:12<00:03,  7.26it/s]\u001b[A\n",
            " 76% 90/118 [00:12<00:03,  7.25it/s]\u001b[A\n",
            " 77% 91/118 [00:12<00:03,  7.26it/s]\u001b[A\n",
            " 78% 92/118 [00:12<00:03,  7.26it/s]\u001b[A\n",
            " 79% 93/118 [00:12<00:03,  7.26it/s]\u001b[A\n",
            " 80% 94/118 [00:12<00:03,  7.26it/s]\u001b[A\n",
            " 81% 95/118 [00:12<00:03,  7.28it/s]\u001b[A\n",
            " 81% 96/118 [00:13<00:03,  7.28it/s]\u001b[A\n",
            " 82% 97/118 [00:13<00:02,  7.26it/s]\u001b[A\n",
            " 83% 98/118 [00:13<00:02,  7.25it/s]\u001b[A\n",
            " 84% 99/118 [00:13<00:02,  7.27it/s]\u001b[A\n",
            " 85% 100/118 [00:13<00:02,  7.26it/s]\u001b[A\n",
            " 86% 101/118 [00:13<00:02,  7.26it/s]\u001b[A\n",
            " 86% 102/118 [00:13<00:02,  7.27it/s]\u001b[A\n",
            " 87% 103/118 [00:14<00:02,  7.26it/s]\u001b[A\n",
            " 88% 104/118 [00:14<00:01,  7.29it/s]\u001b[A\n",
            " 89% 105/118 [00:14<00:01,  7.27it/s]\u001b[A\n",
            " 90% 106/118 [00:14<00:01,  7.25it/s]\u001b[A\n",
            " 91% 107/118 [00:14<00:01,  7.25it/s]\u001b[A\n",
            " 92% 108/118 [00:14<00:01,  7.25it/s]\u001b[A\n",
            " 92% 109/118 [00:14<00:01,  7.26it/s]\u001b[A\n",
            " 93% 110/118 [00:15<00:01,  7.26it/s]\u001b[A\n",
            " 94% 111/118 [00:15<00:00,  7.25it/s]\u001b[A\n",
            " 95% 112/118 [00:15<00:00,  7.26it/s]\u001b[A\n",
            " 96% 113/118 [00:15<00:00,  7.27it/s]\u001b[A\n",
            " 97% 114/118 [00:15<00:00,  7.24it/s]\u001b[A\n",
            " 97% 115/118 [00:15<00:00,  7.24it/s]\u001b[A\n",
            " 98% 116/118 [00:15<00:00,  7.25it/s]\u001b[A\n",
            " 99% 117/118 [00:15<00:00,  7.25it/s]\u001b[A\n",
            "100% 118/118 [00:16<00:00,  7.26it/s]\u001b[AINFO:utils_qa:Post-processing 921 example predictions split into 944 features.\n",
            "\n",
            "\n",
            "  0% 0/921 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  3% 30/921 [00:00<00:02, 299.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "  7% 62/921 [00:00<00:02, 310.64it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10% 95/921 [00:00<00:02, 319.33it/s]\u001b[A\u001b[A\n",
            "\n",
            " 14% 128/921 [00:00<00:02, 320.02it/s]\u001b[A\u001b[A\n",
            "\n",
            " 17% 161/921 [00:00<00:02, 308.41it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21% 195/921 [00:00<00:02, 316.72it/s]\u001b[A\u001b[A\n",
            "\n",
            " 25% 232/921 [00:00<00:02, 330.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 29% 268/921 [00:00<00:01, 337.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33% 303/921 [00:00<00:01, 338.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 37% 339/921 [00:01<00:01, 343.71it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41% 374/921 [00:01<00:01, 329.44it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44% 408/921 [00:01<00:01, 330.73it/s]\u001b[A\u001b[A\n",
            "\n",
            " 48% 443/921 [00:01<00:01, 335.58it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52% 480/921 [00:01<00:01, 344.56it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56% 516/921 [00:01<00:01, 347.23it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60% 553/921 [00:01<00:01, 353.55it/s]\u001b[A\u001b[A\n",
            "\n",
            " 64% 589/921 [00:01<00:00, 341.42it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68% 624/921 [00:01<00:00, 335.65it/s]\u001b[A\u001b[A\n",
            "\n",
            " 71% 658/921 [00:01<00:00, 328.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75% 691/921 [00:02<00:00, 319.65it/s]\u001b[A\u001b[A\n",
            "\n",
            " 79% 726/921 [00:02<00:00, 326.45it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83% 761/921 [00:02<00:00, 331.61it/s]\u001b[A\u001b[A\n",
            "\n",
            " 87% 797/921 [00:02<00:00, 338.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 90% 832/921 [00:02<00:00, 340.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 94% 867/921 [00:02<00:00, 342.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 921/921 [00:02<00:00, 334.41it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                       \n",
            "\u001b[A{'eval_exact_match': 74.48425624321389, 'eval_f1': 85.99821467321424, 'eval_runtime': 16.255, 'eval_samples_per_second': 58.074, 'eval_steps_per_second': 7.259, 'epoch': 2.0}\n",
            " 50% 1924/3848 [25:37<20:35,  1.56it/s]\n",
            "100% 118/118 [00:20<00:00,  7.26it/s]\u001b[A\n",
            "{'loss': 0.812, 'learning_rate': 1.660394265232975e-05, 'epoch': 2.08}\n",
            "{'loss': 0.6078, 'learning_rate': 1.2123655913978495e-05, 'epoch': 2.6}\n",
            " 75% 2886/3848 [38:04<10:20,  1.55it/s][INFO|trainer.py:710] 2023-02-19 17:41:23,257 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 17:41:23,259 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 17:41:23,259 >>   Num examples = 944\n",
            "[INFO|trainer.py:2969] 2023-02-19 17:41:23,259 >>   Batch size = 8\n",
            "\n",
            "  0% 0/118 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/118 [00:00<00:07, 14.60it/s]\u001b[A\n",
            "  3% 4/118 [00:00<00:12,  9.19it/s]\u001b[A\n",
            "  5% 6/118 [00:00<00:13,  8.17it/s]\u001b[A\n",
            "  6% 7/118 [00:00<00:14,  7.91it/s]\u001b[A\n",
            "  7% 8/118 [00:00<00:14,  7.74it/s]\u001b[A\n",
            "  8% 9/118 [00:01<00:14,  7.62it/s]\u001b[A\n",
            "  8% 10/118 [00:01<00:14,  7.51it/s]\u001b[A\n",
            "  9% 11/118 [00:01<00:14,  7.45it/s]\u001b[A\n",
            " 10% 12/118 [00:01<00:14,  7.39it/s]\u001b[A\n",
            " 11% 13/118 [00:01<00:14,  7.37it/s]\u001b[A\n",
            " 12% 14/118 [00:01<00:14,  7.33it/s]\u001b[A\n",
            " 13% 15/118 [00:01<00:14,  7.32it/s]\u001b[A\n",
            " 14% 16/118 [00:02<00:13,  7.31it/s]\u001b[A\n",
            " 14% 17/118 [00:02<00:13,  7.31it/s]\u001b[A\n",
            " 15% 18/118 [00:02<00:13,  7.30it/s]\u001b[A\n",
            " 16% 19/118 [00:02<00:13,  7.29it/s]\u001b[A\n",
            " 17% 20/118 [00:02<00:13,  7.27it/s]\u001b[A\n",
            " 18% 21/118 [00:02<00:13,  7.27it/s]\u001b[A\n",
            " 19% 22/118 [00:02<00:13,  7.27it/s]\u001b[A\n",
            " 19% 23/118 [00:03<00:13,  7.28it/s]\u001b[A\n",
            " 20% 24/118 [00:03<00:12,  7.28it/s]\u001b[A\n",
            " 21% 25/118 [00:03<00:12,  7.28it/s]\u001b[A\n",
            " 22% 26/118 [00:03<00:12,  7.27it/s]\u001b[A\n",
            " 23% 27/118 [00:03<00:12,  7.28it/s]\u001b[A\n",
            " 24% 28/118 [00:03<00:12,  7.29it/s]\u001b[A\n",
            " 25% 29/118 [00:03<00:12,  7.27it/s]\u001b[A\n",
            " 25% 30/118 [00:03<00:12,  7.27it/s]\u001b[A\n",
            " 26% 31/118 [00:04<00:11,  7.28it/s]\u001b[A\n",
            " 27% 32/118 [00:04<00:11,  7.29it/s]\u001b[A\n",
            " 28% 33/118 [00:04<00:11,  7.28it/s]\u001b[A\n",
            " 29% 34/118 [00:04<00:11,  7.29it/s]\u001b[A\n",
            " 30% 35/118 [00:04<00:11,  7.29it/s]\u001b[A\n",
            " 31% 36/118 [00:04<00:11,  7.30it/s]\u001b[A\n",
            " 31% 37/118 [00:04<00:11,  7.29it/s]\u001b[A\n",
            " 32% 38/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 33% 39/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 34% 40/118 [00:05<00:10,  7.27it/s]\u001b[A\n",
            " 35% 41/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 36% 42/118 [00:05<00:10,  7.27it/s]\u001b[A\n",
            " 36% 43/118 [00:05<00:10,  7.27it/s]\u001b[A\n",
            " 37% 44/118 [00:05<00:10,  7.28it/s]\u001b[A\n",
            " 38% 45/118 [00:06<00:10,  7.29it/s]\u001b[A\n",
            " 39% 46/118 [00:06<00:09,  7.28it/s]\u001b[A\n",
            " 40% 47/118 [00:06<00:09,  7.28it/s]\u001b[A\n",
            " 41% 48/118 [00:06<00:09,  7.27it/s]\u001b[A\n",
            " 42% 49/118 [00:06<00:09,  7.28it/s]\u001b[A\n",
            " 42% 50/118 [00:06<00:09,  7.28it/s]\u001b[A\n",
            " 43% 51/118 [00:06<00:09,  7.26it/s]\u001b[A\n",
            " 44% 52/118 [00:07<00:09,  7.26it/s]\u001b[A\n",
            " 45% 53/118 [00:07<00:08,  7.27it/s]\u001b[A\n",
            " 46% 54/118 [00:07<00:08,  7.28it/s]\u001b[A\n",
            " 47% 55/118 [00:07<00:08,  7.26it/s]\u001b[A\n",
            " 47% 56/118 [00:07<00:08,  7.27it/s]\u001b[A\n",
            " 48% 57/118 [00:07<00:08,  7.26it/s]\u001b[A\n",
            " 49% 58/118 [00:07<00:08,  7.27it/s]\u001b[A\n",
            " 50% 59/118 [00:07<00:08,  7.27it/s]\u001b[A\n",
            " 51% 60/118 [00:08<00:08,  7.25it/s]\u001b[A\n",
            " 52% 61/118 [00:08<00:07,  7.25it/s]\u001b[A\n",
            " 53% 62/118 [00:08<00:07,  7.21it/s]\u001b[A\n",
            " 53% 63/118 [00:08<00:07,  7.24it/s]\u001b[A\n",
            " 54% 64/118 [00:08<00:07,  7.26it/s]\u001b[A\n",
            " 55% 65/118 [00:08<00:07,  7.27it/s]\u001b[A\n",
            " 56% 66/118 [00:08<00:07,  7.27it/s]\u001b[A\n",
            " 57% 67/118 [00:09<00:07,  7.25it/s]\u001b[A\n",
            " 58% 68/118 [00:09<00:06,  7.26it/s]\u001b[A\n",
            " 58% 69/118 [00:09<00:06,  7.27it/s]\u001b[A\n",
            " 59% 70/118 [00:09<00:06,  7.25it/s]\u001b[A\n",
            " 60% 71/118 [00:09<00:06,  7.27it/s]\u001b[A\n",
            " 61% 72/118 [00:09<00:06,  7.27it/s]\u001b[A\n",
            " 62% 73/118 [00:09<00:06,  7.24it/s]\u001b[A\n",
            " 63% 74/118 [00:10<00:06,  7.25it/s]\u001b[A\n",
            " 64% 75/118 [00:10<00:05,  7.27it/s]\u001b[A\n",
            " 64% 76/118 [00:10<00:05,  7.28it/s]\u001b[A\n",
            " 65% 77/118 [00:10<00:05,  7.26it/s]\u001b[A\n",
            " 66% 78/118 [00:10<00:05,  7.27it/s]\u001b[A\n",
            " 67% 79/118 [00:10<00:05,  7.29it/s]\u001b[A\n",
            " 68% 80/118 [00:10<00:05,  7.29it/s]\u001b[A\n",
            " 69% 81/118 [00:11<00:05,  7.26it/s]\u001b[A\n",
            " 69% 82/118 [00:11<00:04,  7.26it/s]\u001b[A\n",
            " 70% 83/118 [00:11<00:04,  7.26it/s]\u001b[A\n",
            " 71% 84/118 [00:11<00:04,  7.26it/s]\u001b[A\n",
            " 72% 85/118 [00:11<00:04,  7.26it/s]\u001b[A\n",
            " 73% 86/118 [00:11<00:04,  7.24it/s]\u001b[A\n",
            " 74% 87/118 [00:11<00:04,  7.23it/s]\u001b[A\n",
            " 75% 88/118 [00:11<00:04,  7.22it/s]\u001b[A\n",
            " 75% 89/118 [00:12<00:04,  7.22it/s]\u001b[A\n",
            " 76% 90/118 [00:12<00:03,  7.23it/s]\u001b[A\n",
            " 77% 91/118 [00:12<00:03,  7.22it/s]\u001b[A\n",
            " 78% 92/118 [00:12<00:03,  7.23it/s]\u001b[A\n",
            " 79% 93/118 [00:12<00:03,  7.25it/s]\u001b[A\n",
            " 80% 94/118 [00:12<00:03,  7.26it/s]\u001b[A\n",
            " 81% 95/118 [00:12<00:03,  7.26it/s]\u001b[A\n",
            " 81% 96/118 [00:13<00:03,  7.26it/s]\u001b[A\n",
            " 82% 97/118 [00:13<00:02,  7.25it/s]\u001b[A\n",
            " 83% 98/118 [00:13<00:02,  7.25it/s]\u001b[A\n",
            " 84% 99/118 [00:13<00:02,  7.25it/s]\u001b[A\n",
            " 85% 100/118 [00:13<00:02,  7.21it/s]\u001b[A\n",
            " 86% 101/118 [00:13<00:02,  7.22it/s]\u001b[A\n",
            " 86% 102/118 [00:13<00:02,  7.23it/s]\u001b[A\n",
            " 87% 103/118 [00:14<00:02,  7.25it/s]\u001b[A\n",
            " 88% 104/118 [00:14<00:01,  7.26it/s]\u001b[A\n",
            " 89% 105/118 [00:14<00:01,  7.23it/s]\u001b[A\n",
            " 90% 106/118 [00:14<00:01,  7.22it/s]\u001b[A\n",
            " 91% 107/118 [00:14<00:01,  7.21it/s]\u001b[A\n",
            " 92% 108/118 [00:14<00:01,  7.21it/s]\u001b[A\n",
            " 92% 109/118 [00:14<00:01,  7.23it/s]\u001b[A\n",
            " 93% 110/118 [00:15<00:01,  7.25it/s]\u001b[A\n",
            " 94% 111/118 [00:15<00:00,  7.26it/s]\u001b[A\n",
            " 95% 112/118 [00:15<00:00,  7.27it/s]\u001b[A\n",
            " 96% 113/118 [00:15<00:00,  7.28it/s]\u001b[A\n",
            " 97% 114/118 [00:15<00:00,  7.29it/s]\u001b[A\n",
            " 97% 115/118 [00:15<00:00,  7.29it/s]\u001b[A\n",
            " 98% 116/118 [00:15<00:00,  7.27it/s]\u001b[A\n",
            " 99% 117/118 [00:15<00:00,  7.27it/s]\u001b[A\n",
            "100% 118/118 [00:16<00:00,  7.28it/s]\u001b[AINFO:utils_qa:Post-processing 921 example predictions split into 944 features.\n",
            "\n",
            "\n",
            "  0% 0/921 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4% 35/921 [00:00<00:02, 343.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "  8% 71/921 [00:00<00:02, 347.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 12% 106/921 [00:00<00:02, 347.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 15% 142/921 [00:00<00:02, 348.70it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19% 177/921 [00:00<00:02, 344.65it/s]\u001b[A\u001b[A\n",
            "\n",
            " 23% 213/921 [00:00<00:02, 348.51it/s]\u001b[A\u001b[A\n",
            "\n",
            " 27% 248/921 [00:00<00:01, 348.75it/s]\u001b[A\u001b[A\n",
            "\n",
            " 31% 283/921 [00:00<00:01, 348.52it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35% 318/921 [00:00<00:01, 347.73it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38% 354/921 [00:01<00:01, 349.62it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42% 390/921 [00:01<00:01, 352.68it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46% 426/921 [00:01<00:01, 350.50it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50% 462/921 [00:01<00:01, 346.61it/s]\u001b[A\u001b[A\n",
            "\n",
            " 54% 498/921 [00:01<00:01, 347.74it/s]\u001b[A\u001b[A\n",
            "\n",
            " 58% 533/921 [00:01<00:01, 348.25it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62% 569/921 [00:01<00:01, 349.50it/s]\u001b[A\u001b[A\n",
            "\n",
            " 66% 604/921 [00:01<00:00, 334.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 69% 638/921 [00:01<00:00, 328.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 73% 673/921 [00:01<00:00, 334.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 77% 708/921 [00:02<00:00, 338.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81% 742/921 [00:02<00:00, 337.62it/s]\u001b[A\u001b[A\n",
            "\n",
            " 84% 777/921 [00:02<00:00, 338.82it/s]\u001b[A\u001b[A\n",
            "\n",
            " 88% 814/921 [00:02<00:00, 345.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 92% 849/921 [00:02<00:00, 340.66it/s]\u001b[A\u001b[A\n",
            "\n",
            " 96% 884/921 [00:02<00:00, 337.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 921/921 [00:02<00:00, 342.89it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                       \n",
            "\u001b[A{'eval_exact_match': 74.2671009771987, 'eval_f1': 85.9064263641619, 'eval_runtime': 16.2479, 'eval_samples_per_second': 58.1, 'eval_steps_per_second': 7.262, 'epoch': 3.0}\n",
            " 75% 2886/3848 [38:25<10:20,  1.55it/s]\n",
            "100% 118/118 [00:20<00:00,  7.28it/s]\u001b[A\n",
            "{'loss': 0.5642, 'learning_rate': 7.652329749103942e-06, 'epoch': 3.12}\n",
            "{'loss': 0.4619, 'learning_rate': 3.172043010752688e-06, 'epoch': 3.64}\n",
            "100% 3848/3848 [50:50<00:00,  1.56it/s][INFO|trainer.py:710] 2023-02-19 17:54:09,407 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 17:54:09,409 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 17:54:09,409 >>   Num examples = 944\n",
            "[INFO|trainer.py:2969] 2023-02-19 17:54:09,409 >>   Batch size = 8\n",
            "\n",
            "  0% 0/118 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/118 [00:00<00:07, 14.73it/s]\u001b[A\n",
            "  3% 4/118 [00:00<00:12,  9.23it/s]\u001b[A\n",
            "  5% 6/118 [00:00<00:13,  8.26it/s]\u001b[A\n",
            "  6% 7/118 [00:00<00:13,  7.98it/s]\u001b[A\n",
            "  7% 8/118 [00:00<00:14,  7.80it/s]\u001b[A\n",
            "  8% 9/118 [00:01<00:14,  7.65it/s]\u001b[A\n",
            "  8% 10/118 [00:01<00:14,  7.54it/s]\u001b[A\n",
            "  9% 11/118 [00:01<00:14,  7.46it/s]\u001b[A\n",
            " 10% 12/118 [00:01<00:14,  7.42it/s]\u001b[A\n",
            " 11% 13/118 [00:01<00:14,  7.38it/s]\u001b[A\n",
            " 12% 14/118 [00:01<00:14,  7.37it/s]\u001b[A\n",
            " 13% 15/118 [00:01<00:14,  7.34it/s]\u001b[A\n",
            " 14% 16/118 [00:02<00:13,  7.31it/s]\u001b[A\n",
            " 14% 17/118 [00:02<00:13,  7.31it/s]\u001b[A\n",
            " 15% 18/118 [00:02<00:13,  7.33it/s]\u001b[A\n",
            " 16% 19/118 [00:02<00:13,  7.30it/s]\u001b[A\n",
            " 17% 20/118 [00:02<00:13,  7.30it/s]\u001b[A\n",
            " 18% 21/118 [00:02<00:13,  7.31it/s]\u001b[A\n",
            " 19% 22/118 [00:02<00:13,  7.32it/s]\u001b[A\n",
            " 19% 23/118 [00:03<00:12,  7.31it/s]\u001b[A\n",
            " 20% 24/118 [00:03<00:12,  7.28it/s]\u001b[A\n",
            " 21% 25/118 [00:03<00:12,  7.29it/s]\u001b[A\n",
            " 22% 26/118 [00:03<00:12,  7.30it/s]\u001b[A\n",
            " 23% 27/118 [00:03<00:12,  7.29it/s]\u001b[A\n",
            " 24% 28/118 [00:03<00:12,  7.28it/s]\u001b[A\n",
            " 25% 29/118 [00:03<00:12,  7.24it/s]\u001b[A\n",
            " 25% 30/118 [00:03<00:12,  7.26it/s]\u001b[A\n",
            " 26% 31/118 [00:04<00:11,  7.27it/s]\u001b[A\n",
            " 27% 32/118 [00:04<00:11,  7.23it/s]\u001b[A\n",
            " 28% 33/118 [00:04<00:11,  7.23it/s]\u001b[A\n",
            " 29% 34/118 [00:04<00:11,  7.24it/s]\u001b[A\n",
            " 30% 35/118 [00:04<00:11,  7.26it/s]\u001b[A\n",
            " 31% 36/118 [00:04<00:11,  7.27it/s]\u001b[A\n",
            " 31% 37/118 [00:04<00:11,  7.26it/s]\u001b[A\n",
            " 32% 38/118 [00:05<00:11,  7.19it/s]\u001b[A\n",
            " 33% 39/118 [00:05<00:10,  7.20it/s]\u001b[A\n",
            " 34% 40/118 [00:05<00:10,  7.19it/s]\u001b[A\n",
            " 35% 41/118 [00:05<00:10,  7.21it/s]\u001b[A\n",
            " 36% 42/118 [00:05<00:10,  7.23it/s]\u001b[A\n",
            " 36% 43/118 [00:05<00:10,  7.24it/s]\u001b[A\n",
            " 37% 44/118 [00:05<00:10,  7.24it/s]\u001b[A\n",
            " 38% 45/118 [00:06<00:10,  7.24it/s]\u001b[A\n",
            " 39% 46/118 [00:06<00:09,  7.22it/s]\u001b[A\n",
            " 40% 47/118 [00:06<00:09,  7.25it/s]\u001b[A\n",
            " 41% 48/118 [00:06<00:09,  7.25it/s]\u001b[A\n",
            " 42% 49/118 [00:06<00:09,  7.25it/s]\u001b[A\n",
            " 42% 50/118 [00:06<00:09,  7.24it/s]\u001b[A\n",
            " 43% 51/118 [00:06<00:09,  7.22it/s]\u001b[A\n",
            " 44% 52/118 [00:07<00:09,  7.23it/s]\u001b[A\n",
            " 45% 53/118 [00:07<00:08,  7.25it/s]\u001b[A\n",
            " 46% 54/118 [00:07<00:08,  7.24it/s]\u001b[A\n",
            " 47% 55/118 [00:07<00:08,  7.24it/s]\u001b[A\n",
            " 47% 56/118 [00:07<00:08,  7.21it/s]\u001b[A\n",
            " 48% 57/118 [00:07<00:08,  7.19it/s]\u001b[A\n",
            " 49% 58/118 [00:07<00:08,  7.21it/s]\u001b[A\n",
            " 50% 59/118 [00:07<00:08,  7.20it/s]\u001b[A\n",
            " 51% 60/118 [00:08<00:08,  7.22it/s]\u001b[A\n",
            " 52% 61/118 [00:08<00:07,  7.23it/s]\u001b[A\n",
            " 53% 62/118 [00:08<00:07,  7.25it/s]\u001b[A\n",
            " 53% 63/118 [00:08<00:07,  7.26it/s]\u001b[A\n",
            " 54% 64/118 [00:08<00:07,  7.28it/s]\u001b[A\n",
            " 55% 65/118 [00:08<00:07,  7.27it/s]\u001b[A\n",
            " 56% 66/118 [00:08<00:07,  7.27it/s]\u001b[A\n",
            " 57% 67/118 [00:09<00:07,  7.23it/s]\u001b[A\n",
            " 58% 68/118 [00:09<00:06,  7.25it/s]\u001b[A\n",
            " 58% 69/118 [00:09<00:06,  7.26it/s]\u001b[A\n",
            " 59% 70/118 [00:09<00:06,  7.26it/s]\u001b[A\n",
            " 60% 71/118 [00:09<00:06,  7.26it/s]\u001b[A\n",
            " 61% 72/118 [00:09<00:06,  7.26it/s]\u001b[A\n",
            " 62% 73/118 [00:09<00:06,  7.27it/s]\u001b[A\n",
            " 63% 74/118 [00:10<00:06,  7.28it/s]\u001b[A\n",
            " 64% 75/118 [00:10<00:05,  7.25it/s]\u001b[A\n",
            " 64% 76/118 [00:10<00:05,  7.27it/s]\u001b[A\n",
            " 65% 77/118 [00:10<00:05,  7.24it/s]\u001b[A\n",
            " 66% 78/118 [00:10<00:05,  7.25it/s]\u001b[A\n",
            " 67% 79/118 [00:10<00:05,  7.24it/s]\u001b[A\n",
            " 68% 80/118 [00:10<00:05,  7.26it/s]\u001b[A\n",
            " 69% 81/118 [00:11<00:05,  7.27it/s]\u001b[A\n",
            " 69% 82/118 [00:11<00:04,  7.28it/s]\u001b[A\n",
            " 70% 83/118 [00:11<00:04,  7.29it/s]\u001b[A\n",
            " 71% 84/118 [00:11<00:04,  7.28it/s]\u001b[A\n",
            " 72% 85/118 [00:11<00:04,  7.28it/s]\u001b[A\n",
            " 73% 86/118 [00:11<00:04,  7.25it/s]\u001b[A\n",
            " 74% 87/118 [00:11<00:04,  7.27it/s]\u001b[A\n",
            " 75% 88/118 [00:11<00:04,  7.25it/s]\u001b[A\n",
            " 75% 89/118 [00:12<00:03,  7.25it/s]\u001b[A\n",
            " 76% 90/118 [00:12<00:03,  7.22it/s]\u001b[A\n",
            " 77% 91/118 [00:12<00:03,  7.24it/s]\u001b[A\n",
            " 78% 92/118 [00:12<00:03,  7.27it/s]\u001b[A\n",
            " 79% 93/118 [00:12<00:03,  7.27it/s]\u001b[A\n",
            " 80% 94/118 [00:12<00:03,  7.26it/s]\u001b[A\n",
            " 81% 95/118 [00:12<00:03,  7.28it/s]\u001b[A\n",
            " 81% 96/118 [00:13<00:03,  7.29it/s]\u001b[A\n",
            " 82% 97/118 [00:13<00:02,  7.30it/s]\u001b[A\n",
            " 83% 98/118 [00:13<00:02,  7.30it/s]\u001b[A\n",
            " 84% 99/118 [00:13<00:02,  7.30it/s]\u001b[A\n",
            " 85% 100/118 [00:13<00:02,  7.30it/s]\u001b[A\n",
            " 86% 101/118 [00:13<00:02,  7.31it/s]\u001b[A\n",
            " 86% 102/118 [00:13<00:02,  7.30it/s]\u001b[A\n",
            " 87% 103/118 [00:14<00:02,  7.29it/s]\u001b[A\n",
            " 88% 104/118 [00:14<00:01,  7.29it/s]\u001b[A\n",
            " 89% 105/118 [00:14<00:01,  7.26it/s]\u001b[A\n",
            " 90% 106/118 [00:14<00:01,  7.27it/s]\u001b[A\n",
            " 91% 107/118 [00:14<00:01,  7.27it/s]\u001b[A\n",
            " 92% 108/118 [00:14<00:01,  7.28it/s]\u001b[A\n",
            " 92% 109/118 [00:14<00:01,  7.30it/s]\u001b[A\n",
            " 93% 110/118 [00:15<00:01,  7.30it/s]\u001b[A\n",
            " 94% 111/118 [00:15<00:00,  7.29it/s]\u001b[A\n",
            " 95% 112/118 [00:15<00:00,  7.27it/s]\u001b[A\n",
            " 96% 113/118 [00:15<00:00,  7.27it/s]\u001b[A\n",
            " 97% 114/118 [00:15<00:00,  7.27it/s]\u001b[A\n",
            " 97% 115/118 [00:15<00:00,  7.27it/s]\u001b[A\n",
            " 98% 116/118 [00:15<00:00,  7.22it/s]\u001b[A\n",
            " 99% 117/118 [00:15<00:00,  7.21it/s]\u001b[A\n",
            "100% 118/118 [00:16<00:00,  7.24it/s]\u001b[AINFO:utils_qa:Post-processing 921 example predictions split into 944 features.\n",
            "\n",
            "\n",
            "  0% 0/921 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  3% 32/921 [00:00<00:02, 316.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "  7% 66/921 [00:00<00:02, 330.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 11% 101/921 [00:00<00:02, 337.80it/s]\u001b[A\u001b[A\n",
            "\n",
            " 15% 135/921 [00:00<00:02, 334.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 18% 169/921 [00:00<00:02, 332.32it/s]\u001b[A\u001b[A\n",
            "\n",
            " 22% 203/921 [00:00<00:02, 332.22it/s]\u001b[A\u001b[A\n",
            "\n",
            " 26% 237/921 [00:00<00:02, 331.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 29% 271/921 [00:00<00:02, 323.81it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33% 304/921 [00:00<00:01, 322.59it/s]\u001b[A\u001b[A\n",
            "\n",
            " 37% 340/921 [00:01<00:01, 331.47it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41% 374/921 [00:01<00:01, 322.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44% 407/921 [00:01<00:01, 322.29it/s]\u001b[A\u001b[A\n",
            "\n",
            " 48% 440/921 [00:01<00:01, 323.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51% 473/921 [00:01<00:01, 318.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55% 508/921 [00:01<00:01, 327.23it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59% 543/921 [00:01<00:01, 332.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63% 577/921 [00:01<00:01, 321.62it/s]\u001b[A\u001b[A\n",
            "\n",
            " 66% 610/921 [00:01<00:00, 312.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70% 642/921 [00:01<00:00, 306.70it/s]\u001b[A\u001b[A\n",
            "\n",
            " 73% 673/921 [00:02<00:00, 293.84it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76% 703/921 [00:02<00:00, 293.64it/s]\u001b[A\u001b[A\n",
            "\n",
            " 80% 734/921 [00:02<00:00, 295.85it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83% 767/921 [00:02<00:00, 303.36it/s]\u001b[A\u001b[A\n",
            "\n",
            " 87% 801/921 [00:02<00:00, 312.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 90% 833/921 [00:02<00:00, 298.67it/s]\u001b[A\u001b[A\n",
            "\n",
            " 94% 865/921 [00:02<00:00, 304.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 921/921 [00:02<00:00, 314.25it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                       \n",
            "\u001b[A{'eval_exact_match': 75.57003257328991, 'eval_f1': 87.21694957524456, 'eval_runtime': 16.2461, 'eval_samples_per_second': 58.106, 'eval_steps_per_second': 7.263, 'epoch': 4.0}\n",
            "100% 3848/3848 [51:11<00:00,  1.56it/s]\n",
            "100% 118/118 [00:20<00:00,  7.24it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1901] 2023-02-19 17:54:29,977 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3071.5795, 'train_samples_per_second': 20.03, 'train_steps_per_second': 1.253, 'train_loss': 0.9937688704587814, 'epoch': 4.0}\n",
            "100% 3848/3848 [51:11<00:00,  1.25it/s]\n",
            "[INFO|trainer.py:2709] 2023-02-19 17:54:29,979 >> Saving model checkpoint to out\n",
            "[INFO|configuration_utils.py:453] 2023-02-19 17:54:29,979 >> Configuration saved in out/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-19 17:54:32,026 >> Model weights saved in out/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-19 17:54:32,027 >> tokenizer config file saved in out/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-19 17:54:32,027 >> Special tokens file saved in out/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        4.0\n",
            "  train_loss               =     0.9938\n",
            "  train_runtime            = 0:51:11.57\n",
            "  train_samples            =      15381\n",
            "  train_samples_per_second =      20.03\n",
            "  train_steps_per_second   =      1.253\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:710] 2023-02-19 17:54:32,125 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 17:54:32,127 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 17:54:32,127 >>   Num examples = 944\n",
            "[INFO|trainer.py:2969] 2023-02-19 17:54:32,127 >>   Batch size = 8\n",
            "100% 118/118 [00:16<00:00,  7.25it/s]INFO:utils_qa:Post-processing 921 example predictions split into 944 features.\n",
            "\n",
            "  0% 0/921 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 34/921 [00:00<00:02, 331.44it/s]\u001b[A\n",
            "  7% 68/921 [00:00<00:02, 334.40it/s]\u001b[A\n",
            " 11% 103/921 [00:00<00:02, 339.30it/s]\u001b[A\n",
            " 15% 137/921 [00:00<00:02, 339.10it/s]\u001b[A\n",
            " 19% 172/921 [00:00<00:02, 341.38it/s]\u001b[A\n",
            " 22% 207/921 [00:00<00:02, 337.67it/s]\u001b[A\n",
            " 26% 243/921 [00:00<00:01, 342.71it/s]\u001b[A\n",
            " 30% 279/921 [00:00<00:01, 345.49it/s]\u001b[A\n",
            " 34% 314/921 [00:00<00:01, 346.09it/s]\u001b[A\n",
            " 38% 350/921 [00:01<00:01, 349.04it/s]\u001b[A\n",
            " 42% 386/921 [00:01<00:01, 350.23it/s]\u001b[A\n",
            " 46% 422/921 [00:01<00:01, 351.03it/s]\u001b[A\n",
            " 50% 458/921 [00:01<00:01, 344.93it/s]\u001b[A\n",
            " 54% 494/921 [00:01<00:01, 347.89it/s]\u001b[A\n",
            " 58% 530/921 [00:01<00:01, 350.59it/s]\u001b[A\n",
            " 61% 566/921 [00:01<00:01, 346.97it/s]\u001b[A\n",
            " 65% 601/921 [00:01<00:00, 333.87it/s]\u001b[A\n",
            " 69% 635/921 [00:01<00:00, 324.99it/s]\u001b[A\n",
            " 73% 669/921 [00:01<00:00, 329.11it/s]\u001b[A\n",
            " 76% 703/921 [00:02<00:00, 330.49it/s]\u001b[A\n",
            " 80% 738/921 [00:02<00:00, 334.43it/s]\u001b[A\n",
            " 84% 773/921 [00:02<00:00, 338.83it/s]\u001b[A\n",
            " 88% 810/921 [00:02<00:00, 347.04it/s]\u001b[A\n",
            " 92% 845/921 [00:02<00:00, 343.97it/s]\u001b[A\n",
            " 96% 880/921 [00:02<00:00, 339.46it/s]\u001b[A\n",
            "100% 921/921 [00:02<00:00, 340.24it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "100% 118/118 [00:20<00:00,  5.86it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        4.0\n",
            "  eval_exact_match        =      75.57\n",
            "  eval_f1                 =    87.2169\n",
            "  eval_runtime            = 0:00:16.28\n",
            "  eval_samples            =        944\n",
            "  eval_samples_per_second =     57.969\n",
            "  eval_steps_per_second   =      7.246\n",
            "[INFO|modelcard.py:449] 2023-02-19 17:54:53,241 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'artydiqa.py secondary_task', 'type': 'artydiqa.py', 'config': 'secondary_task', 'split': 'validation', 'args': 'secondary_task'}}\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/transformers/examples/pytorch/question-answering/run_qa.py \\\n",
        "\t--output_dir out \\\n",
        "\t--model_name_or_path sultan/ArabicTransformer-intermediate \\\n",
        "\t--dataset_name artydiqa.py \\\n",
        "\t--dataset_config_name \"secondary_task\" \\\n",
        "\t--do_train --do_eval \\\n",
        "\t--per_device_train_batch_size 16 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 4 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir out \\\n",
        "  --n_best_size 20 \\\n",
        "  --evaluation_strategy epoch \\\n",
        "  --save_steps 500000 \\\n",
        "  --overwrite_output_dir \\\n",
        "  --seed 42 \\\n",
        "  --warmup_steps 500 \\\n",
        "\t--fp16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the evaluation results, we achieved 87.22/75.57 F1/EM scores which slightly exceeded our reported results in the paper."
      ],
      "metadata": {
        "id": "Vy_XSsyp0yEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quran_Qa dataset**"
      ],
      "metadata": {
        "id": "l27tIKwx_8Lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://sites.google.com/view/quran-qa-2022"
      ],
      "metadata": {
        "id": "XHO_YLV4ACXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O qrcd_v1.1_dev.jsonl https://gitlab.com/bigirqu/quranqa/-/raw/main/datasets/qrcd_v1.1_dev.jsonl?inline=false\n",
        "!wget -O qrcd_v1.1_train.jsonl https://gitlab.com/bigirqu/quranqa/-/raw/main/datasets/qrcd_v1.1_train.jsonl?inline=false\n",
        "!wget -O qrcd_v1.1_test.jsonl https://gitlab.com/bigirqu/quranqa/-/raw/main/datasets/qrcd_v1.1_test_gold.jsonl?inline=false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6C4FviKs_WN",
        "outputId": "734e2cb8-e148-487e-c14d-16dd9da27121"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-19 18:10:55--  https://gitlab.com/bigirqu/quranqa/-/raw/main/datasets/qrcd_v1.1_dev.jsonl?inline=false\n",
            "Resolving gitlab.com (gitlab.com)... 172.65.251.78, 2606:4700:90:0:f22e:fbec:5bed:a9b9\n",
            "Connecting to gitlab.com (gitlab.com)|172.65.251.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 117412 (115K) [application/octet-stream]\n",
            "Saving to: ‘qrcd_v1.1_dev.jsonl’\n",
            "\n",
            "qrcd_v1.1_dev.jsonl 100%[===================>] 114.66K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-02-19 18:10:56 (4.64 MB/s) - ‘qrcd_v1.1_dev.jsonl’ saved [117412/117412]\n",
            "\n",
            "--2023-02-19 18:10:56--  https://gitlab.com/bigirqu/quranqa/-/raw/main/datasets/qrcd_v1.1_train.jsonl?inline=false\n",
            "Resolving gitlab.com (gitlab.com)... 172.65.251.78, 2606:4700:90:0:f22e:fbec:5bed:a9b9\n",
            "Connecting to gitlab.com (gitlab.com)|172.65.251.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 756171 (738K) [application/octet-stream]\n",
            "Saving to: ‘qrcd_v1.1_train.jsonl’\n",
            "\n",
            "qrcd_v1.1_train.jso 100%[===================>] 738.45K  1.40MB/s    in 0.5s    \n",
            "\n",
            "2023-02-19 18:10:57 (1.40 MB/s) - ‘qrcd_v1.1_train.jsonl’ saved [756171/756171]\n",
            "\n",
            "--2023-02-19 18:10:57--  https://gitlab.com/bigirqu/quranqa/-/raw/main/datasets/qrcd_v1.1_test_gold.jsonl?inline=false\n",
            "Resolving gitlab.com (gitlab.com)... 172.65.251.78, 2606:4700:90:0:f22e:fbec:5bed:a9b9\n",
            "Connecting to gitlab.com (gitlab.com)|172.65.251.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 238439 (233K) [application/octet-stream]\n",
            "Saving to: ‘qrcd_v1.1_test.jsonl’\n",
            "\n",
            "qrcd_v1.1_test.json 100%[===================>] 232.85K   824KB/s    in 0.3s    \n",
            "\n",
            "2023-02-19 18:10:57 (824 KB/s) - ‘qrcd_v1.1_test.jsonl’ saved [238439/238439]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd \n",
        "def quran_jsonl_to_json(quran_data,squad_style_file):\n",
        "  quaran_pd_data=pd.read_json(quran_data,lines=True)\n",
        "  quran_qa_json_entry=[]\n",
        "  for index,row in quaran_pd_data.iterrows():\n",
        "      quran_qa_json_entry.append({\"title\":\"surah\",\"paragraphs\":[{\"qas\":[{\"question\":row[\"question\"], \"id\":row[\"pq_id\"],\"answers\":row[\"answers\"]}],\"context\":row[\"passage\"]}]})\n",
        "  with open(squad_style_file, 'w') as fp:\n",
        "      json.dump({\"data\":quran_qa_json_entry,\"version\":\"Quran_Dataset_SQuAD_Style1.1\"}, fp)\n",
        "quran_jsonl_to_json(\"qrcd_v1.1_train.jsonl\",\"qrcd_v1.1_train.json\")\n",
        "quran_jsonl_to_json(\"qrcd_v1.1_dev.jsonl\",\"qrcd_v1.1_dev.json\")\n",
        "quran_jsonl_to_json(\"qrcd_v1.1_test.jsonl\",\"qrcd_v1.1_test.json\")\n",
        "## we need to replace \"start_char\" key in quran_qa dataset with \"answer_start\" key to match tydiqa and squad style.\n",
        "!sed -i 's/start_char/answer_start/g' qrcd_v1.1_train.json\n",
        "!sed -i 's/start_char/answer_start/g' qrcd_v1.1_dev.json\n",
        "!sed -i 's/start_char/answer_start/g' qrcd_v1.1_test.json"
      ],
      "metadata": {
        "id": "DSCgVmM91vDx"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "click on the play button on the code below so the code will saved to this colab as \"quran_qa.py\""
      ],
      "metadata": {
        "id": "MrrHI6m-_t8V"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceeaaeae-54d3-4ffa-eca5-ccf40f5aba4e",
        "cellView": "form",
        "id": "Okrao2GW3-dK"
      },
      "source": [
        "#@title\n",
        "%%writefile quran_qa.py\n",
        "\"\"\"TODO(tydiqa): Add a description here.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "import datasets\n",
        "\n",
        "\n",
        "# TODO(tydiqa): BibTeX citation\n",
        "_CITATION = \"\"\"\\\n",
        "@article{tydiqa,\n",
        "title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},\n",
        "author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}\n",
        "year    = {2020},\n",
        "journal = {Transactions of the Association for Computational Linguistics}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# TODO(tydiqa):\n",
        "_DESCRIPTION = \"\"\"\\\n",
        "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\n",
        "The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\n",
        "expresses -- such that we expect models performing well on this set to generalize across a large number of the languages\n",
        "in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic\n",
        "information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but\n",
        "don’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\n",
        "the use of translation (unlike MLQA and XQuAD).\n",
        "\"\"\"\n",
        "\n",
        "# _URL = \"https://storage.googleapis.com/tydiqa/\"\n",
        "\n",
        "# _PRIMARY_URLS = {\n",
        "#     \"train\": _URL + \"v1.0/tydiqa-v1.0-train.jsonl.gz\",\n",
        "#     \"dev\": _URL + \"v1.0/tydiqa-v1.0-dev.jsonl.gz\",\n",
        "# }\n",
        "# _SECONDARY_URLS = {\n",
        "#     \"train\": _URL + \"v1.1/tydiqa-goldp-v1.1-train.json\",\n",
        "#     \"dev\": _URL + \"v1.1/tydiqa-goldp-v1.1-dev.json\",\n",
        "# }\n",
        "\n",
        "#use this for AraBERTv1 and V2\n",
        "_URL = \"https://storage.googleapis.com/tydiqa/\"\n",
        "_URL2 = \"/content/\"\n",
        "_PRIMARY_URLS = {\n",
        "    \"train\": _URL + \"v1.0/tydiqa-v1.0-train.jsonl.gz\",\n",
        "    \"dev\": _URL + \"v1.0/tydiqa-v1.0-dev.jsonl.gz\",\n",
        "}\n",
        "_SECONDARY_URLS = {\n",
        "    \"train\": _URL2 + \"qrcd_v1.1_train.json\",\n",
        "    \"dev\": _URL2 + \"qrcd_v1.1_dev.json\",\n",
        "    \"test\": _URL2 + \"qrcd_v1.1_test.json\",\n",
        "}\n",
        "\n",
        "\n",
        "class TydiqaConfig(datasets.BuilderConfig):\n",
        "\n",
        "    \"\"\" BuilderConfig for Tydiqa\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            **kwargs: keyword arguments forwarded to super.\n",
        "        \"\"\"\n",
        "        super(TydiqaConfig, self).__init__(version=datasets.Version(\"1.0.0\", \"\"), **kwargs)\n",
        "\n",
        "\n",
        "class Tydiqa(datasets.GeneratorBasedBuilder):\n",
        "    \"\"\"TODO(tydiqa): Short description of my dataset.\"\"\"\n",
        "\n",
        "    # TODO(tydiqa): Set up version.\n",
        "    VERSION = datasets.Version(\"0.1.0\")\n",
        "    BUILDER_CONFIGS = [\n",
        "        TydiqaConfig(\n",
        "            name=\"primary_task\",\n",
        "            description=textwrap.dedent(\n",
        "                \"\"\"\\\n",
        "          Passage selection task (SelectP): Given a list of the passages in the article, return either (a) the index of\n",
        "          the passage that answers the question or (b) NULL if no such passage exists.\n",
        "          Minimal answer span task (MinSpan): Given the full text of an article, return one of (a) the start and end\n",
        "          byte indices of the minimal span that completely answers the question; (b) YES or NO if the question requires\n",
        "          a yes/no answer and we can draw a conclusion from the passage; (c) NULL if it is not possible to produce a\n",
        "          minimal answer for this question.\"\"\"\n",
        "            ),\n",
        "        ),\n",
        "        TydiqaConfig(\n",
        "            name=\"secondary_task\",\n",
        "            description=textwrap.dedent(\n",
        "                \"\"\"Gold passage task (GoldP): Given a passage that is guaranteed to contain the\n",
        "          answer, predict the single contiguous span of characters that answers the question. This is more similar to\n",
        "          existing reading comprehension datasets (as opposed to the information-seeking task outlined above).\n",
        "          This task is constructed with two goals in mind: (1) more directly comparing with prior work and (2) providing\n",
        "          a simplified way for researchers to use TyDi QA by providing compatibility with existing code for SQuAD 1.1,\n",
        "          XQuAD, and MLQA. Toward these goals, the gold passage task differs from the primary task in several ways:\n",
        "          only the gold answer passage is provided rather than the entire Wikipedia article;\n",
        "          unanswerable questions have been discarded, similar to MLQA and XQuAD;\n",
        "          we evaluate with the SQuAD 1.1 metrics like XQuAD; and\n",
        "         Thai and Japanese are removed since the lack of whitespace breaks some tools.\n",
        "          \"\"\"\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    def _info(self):\n",
        "        # TODO(tydiqa): Specifies the datasets.DatasetInfo object\n",
        "        if self.config.name == \"primary_task\":\n",
        "            return datasets.DatasetInfo(\n",
        "                # This is the description that will appear on the datasets page.\n",
        "                description=_DESCRIPTION,\n",
        "                # datasets.features.FeatureConnectors\n",
        "                features=datasets.Features(\n",
        "                    {\n",
        "                        \"passage_answer_candidates\": datasets.features.Sequence(\n",
        "                            {\n",
        "                                \"plaintext_start_byte\": datasets.Value(\"int32\"),\n",
        "                                \"plaintext_end_byte\": datasets.Value(\"int32\"),\n",
        "                            }\n",
        "                        ),\n",
        "                        \"question_text\": datasets.Value(\"string\"),\n",
        "                        \"document_title\": datasets.Value(\"string\"),\n",
        "                        \"language\": datasets.Value(\"string\"),\n",
        "                        \"annotations\": datasets.features.Sequence(\n",
        "                            {\n",
        "                                # 'annotation_id': datasets.Value('variant'),\n",
        "                                \"passage_answer_candidate_index\": datasets.Value(\"int32\"),\n",
        "                                \"minimal_answers_start_byte\": datasets.Value(\"int32\"),\n",
        "                                \"minimal_answers_end_byte\": datasets.Value(\"int32\"),\n",
        "                                \"yes_no_answer\": datasets.Value(\"string\"),\n",
        "                            }\n",
        "                        ),\n",
        "                        \"document_plaintext\": datasets.Value(\"string\"),\n",
        "                        # 'example_id': datasets.Value('variant'),\n",
        "                        \"document_url\": datasets.Value(\"string\")\n",
        "                        # These are the features of your dataset like images, labels ...\n",
        "                    }\n",
        "                ),\n",
        "                # If there's a common (input, target) tuple from the features,\n",
        "                # specify them here. They'll be used if as_supervised=True in\n",
        "                # builder.as_dataset.\n",
        "                supervised_keys=None,\n",
        "                # Homepage of the dataset for documentation\n",
        "                homepage=\"https://github.com/google-research-datasets/tydiqa\",\n",
        "                citation=_CITATION,\n",
        "            )\n",
        "        elif self.config.name == \"secondary_task\":\n",
        "            return datasets.DatasetInfo(\n",
        "                description=_DESCRIPTION,\n",
        "                features=datasets.Features(\n",
        "                    {\n",
        "                        \"id\": datasets.Value(\"string\"),\n",
        "                        \"title\": datasets.Value(\"string\"),\n",
        "                        \"context\": datasets.Value(\"string\"),\n",
        "                        \"question\": datasets.Value(\"string\"),\n",
        "                        \"answers\": datasets.features.Sequence(\n",
        "                            {\n",
        "                                \"text\": datasets.Value(\"string\"),\n",
        "                                \"answer_start\": datasets.Value(\"int32\"),\n",
        "                            }\n",
        "                        ),\n",
        "                    }\n",
        "                ),\n",
        "                # No default supervised_keys (as we have to pass both question\n",
        "                # and context as input).\n",
        "                supervised_keys=None,\n",
        "                homepage=\"https://github.com/google-research-datasets/tydiqa\",\n",
        "                citation=_CITATION,\n",
        "            )\n",
        "\n",
        "    def _split_generators(self, dl_manager):\n",
        "        \"\"\"Returns SplitGenerators.\"\"\"\n",
        "        # TODO(tydiqa): Downloads the data and defines the splits\n",
        "        # dl_manager is a datasets.download.DownloadManager that can be used to\n",
        "        # download and extract URLs\n",
        "        primary_downloaded = dl_manager.download_and_extract(_PRIMARY_URLS)\n",
        "        secondary_downloaded = dl_manager.download_and_extract(_SECONDARY_URLS)\n",
        "        if self.config.name == \"primary_task\":\n",
        "            return [\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.TRAIN,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": primary_downloaded[\"train\"]},\n",
        "                ),\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.VALIDATION,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": primary_downloaded[\"dev\"]},\n",
        "                ),\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.TEST,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": primary_downloaded[\"test\"]},\n",
        "                ),\n",
        "            ]\n",
        "        elif self.config.name == \"secondary_task\":\n",
        "            return [\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.TRAIN,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": secondary_downloaded[\"train\"]},\n",
        "                ),\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.VALIDATION,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": secondary_downloaded[\"dev\"]},\n",
        "                ),\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.TEST,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": secondary_downloaded[\"test\"]},\n",
        "                ),\n",
        "            ]\n",
        "\n",
        "    def _generate_examples(self, filepath):\n",
        "        \"\"\"Yields examples.\"\"\"\n",
        "        # TODO(tydiqa): Yields (key, example) tuples from the dataset\n",
        "        if self.config.name == \"primary_task\":\n",
        "            with open(filepath, encoding=\"utf-8\") as f:\n",
        "                for id_, row in enumerate(f):\n",
        "                    data = json.loads(row)\n",
        "                    passages = data[\"passage_answer_candidates\"]\n",
        "                    end_byte = [passage[\"plaintext_end_byte\"] for passage in passages]\n",
        "                    start_byte = [passage[\"plaintext_start_byte\"] for passage in passages]\n",
        "                    title = data[\"document_title\"]\n",
        "                    lang = data[\"language\"]\n",
        "                    question = data[\"question_text\"]\n",
        "                    annotations = data[\"annotations\"]\n",
        "                    # annot_ids = [annotation[\"annotation_id\"] for annotation in annotations]\n",
        "                    yes_no_answers = [annotation[\"yes_no_answer\"] for annotation in annotations]\n",
        "                    min_answers_end_byte = [\n",
        "                        annotation[\"minimal_answer\"][\"plaintext_end_byte\"] for annotation in annotations\n",
        "                    ]\n",
        "                    min_answers_start_byte = [\n",
        "                        annotation[\"minimal_answer\"][\"plaintext_start_byte\"] for annotation in annotations\n",
        "                    ]\n",
        "                    passage_cand_answers = [\n",
        "                        annotation[\"passage_answer\"][\"candidate_index\"] for annotation in annotations\n",
        "                    ]\n",
        "                    doc = data[\"document_plaintext\"]\n",
        "                    # example_id = data[\"example_id\"]\n",
        "                    url = data[\"document_url\"]\n",
        "                    yield id_, {\n",
        "                        \"passage_answer_candidates\": {\n",
        "                            \"plaintext_start_byte\": start_byte,\n",
        "                            \"plaintext_end_byte\": end_byte,\n",
        "                        },\n",
        "                        \"question_text\": question,\n",
        "                        \"document_title\": title,\n",
        "                        \"language\": lang,\n",
        "                        \"annotations\": {\n",
        "                            # 'annotation_id': annot_ids,\n",
        "                            \"passage_answer_candidate_index\": passage_cand_answers,\n",
        "                            \"minimal_answers_start_byte\": min_answers_start_byte,\n",
        "                            \"minimal_answers_end_byte\": min_answers_end_byte,\n",
        "                            \"yes_no_answer\": yes_no_answers,\n",
        "                        },\n",
        "                        \"document_plaintext\": doc,\n",
        "                        # 'example_id': example_id,\n",
        "                        \"document_url\": url,\n",
        "                    }\n",
        "        elif self.config.name == \"secondary_task\":\n",
        "            with open(filepath, encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "                for article in data[\"data\"]:\n",
        "                    title = article.get(\"title\", \"\").strip()\n",
        "                    for paragraph in article[\"paragraphs\"]:\n",
        "                        context = paragraph[\"context\"].strip()\n",
        "                        for qa in paragraph[\"qas\"]:\n",
        "                            question = qa[\"question\"].strip()\n",
        "                            id_ = qa[\"id\"]\n",
        "                            answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"] if answer[\"answer_start\"] != -1]                            \n",
        "                            answers = [answer[\"text\"].strip() for answer in qa[\"answers\"] if answer[\"answer_start\"] != -1]\n",
        "                            if len(answers) == 0 or len(answer_starts)==0:\n",
        "                              print(\"question skipped\")\n",
        "                              continue\n",
        "                            assert len(answer_starts)==len(answers)\n",
        "                            # Features currently used are \"context\", \"question\", and \"answers\".\n",
        "                            # Others are extracted here for the ease of future expansions.\n",
        "                            yield id_, {\n",
        "                                \"title\": title,\n",
        "                                \"context\": context,\n",
        "                                \"question\": question,\n",
        "                                \"id\": id_,\n",
        "                                \"answers\": {\n",
        "                                    \"answer_start\": answer_starts,\n",
        "                                    \"text\": answers,\n",
        "                                },\n",
        "                            }"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting quran_qa.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since quran_qa dataset is small, you will use the checkpoint that we finetuned on ArabicTyDi QA dataset from previous step (we saved it to [out] directory)"
      ],
      "metadata": {
        "id": "CZD7OwVTEk6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/transformers/examples/pytorch/question-answering/run_qa.py \\\n",
        "\t--output_dir quran_qa_model \\\n",
        "\t--model_name_or_path out \\\n",
        "\t--dataset_name quran_qa.py \\\n",
        "\t--dataset_config_name \"secondary_task\" \\\n",
        "\t--do_train --do_eval --do_predict \\\n",
        "\t--per_device_train_batch_size 16 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 10 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir out \\\n",
        "  --n_best_size 20 \\\n",
        "  --evaluation_strategy epoch \\\n",
        "  --save_steps 500000 \\\n",
        "  --overwrite_output_dir \\\n",
        "  --overwrite_cache \\\n",
        "  --seed 42 \\\n",
        "  --warmup_steps 500 \\\n",
        "\t--fp16 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXBOHziM4Wmo",
        "outputId": "764edce9-6420-4566-daa1-54d0a16b0114"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-19 18:36:34.644681: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-19 18:36:35.584964: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-19 18:36:35.585071: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-19 18:36:35.585088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/runs/Feb19_18-36-38_c015a1dc4b85,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=out,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out,\n",
            "save_on_each_node=False,\n",
            "save_steps=500000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=500,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/quran_qa/e84fb1664f219bac6ebbd7defed5ceb244433c5d7a5d62fdad09293702373e53\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/quran_qa/secondary_task/1.0.0/e84fb1664f219bac6ebbd7defed5ceb244433c5d7a5d62fdad09293702373e53\n",
            "WARNING:datasets.builder:Found cached dataset quran_qa (/root/.cache/huggingface/datasets/quran_qa/secondary_task/1.0.0/e84fb1664f219bac6ebbd7defed5ceb244433c5d7a5d62fdad09293702373e53)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/quran_qa/secondary_task/1.0.0/e84fb1664f219bac6ebbd7defed5ceb244433c5d7a5d62fdad09293702373e53\n",
            "100% 3/3 [00:00<00:00, 765.34it/s]\n",
            "[INFO|configuration_utils.py:658] 2023-02-19 18:36:40,058 >> loading configuration file out/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-19 18:36:40,059 >> Model config FunnelConfig {\n",
            "  \"_name_or_path\": \"out\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"architectures\": [\n",
            "    \"FunnelForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_type\": \"relative_shift\",\n",
            "  \"block_repeats\": [\n",
            "    1,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"block_sizes\": [\n",
            "    6,\n",
            "    6,\n",
            "    6\n",
            "  ],\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"initializer_range\": 0.1,\n",
            "  \"initializer_std\": null,\n",
            "  \"layer_norm_eps\": 1e-09,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"funnel\",\n",
            "  \"n_head\": 12,\n",
            "  \"num_decoder_layers\": 2,\n",
            "  \"pool_q_only\": true,\n",
            "  \"pooling_type\": \"mean\",\n",
            "  \"rel_attn_type\": \"factorized\",\n",
            "  \"separate_cls\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"truncate_seq\": true,\n",
            "  \"type_vocab_size\": 3,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1800] 2023-02-19 18:36:40,060 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1800] 2023-02-19 18:36:40,060 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1800] 2023-02-19 18:36:40,060 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1800] 2023-02-19 18:36:40,060 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1800] 2023-02-19 18:36:40,060 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2272] 2023-02-19 18:36:40,109 >> loading weights file out/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2857] 2023-02-19 18:36:42,165 >> All model checkpoint weights were used when initializing FunnelForQuestionAnswering.\n",
            "\n",
            "[INFO|modeling_utils.py:2865] 2023-02-19 18:36:42,165 >> All the weights of FunnelForQuestionAnswering were initialized from the model checkpoint at out.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use FunnelForQuestionAnswering for predictions without further training.\n",
            "Running tokenizer on train dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/quran_qa/secondary_task/1.0.0/e84fb1664f219bac6ebbd7defed5ceb244433c5d7a5d62fdad09293702373e53/cache-472a266942b9e951.arrow\n",
            "Running tokenizer on train dataset: 100% 1/1 [00:00<00:00,  3.33ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/quran_qa/secondary_task/1.0.0/e84fb1664f219bac6ebbd7defed5ceb244433c5d7a5d62fdad09293702373e53/cache-0621cf3eb7a2dc11.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00, 18.03ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/quran_qa/secondary_task/1.0.0/e84fb1664f219bac6ebbd7defed5ceb244433c5d7a5d62fdad09293702373e53/cache-4e8869a2e96a8c2e.arrow\n",
            "Running tokenizer on prediction dataset: 100% 1/1 [00:00<00:00,  9.10ba/s]\n",
            "[INFO|trainer.py:565] 2023-02-19 18:36:46,745 >> Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1650] 2023-02-19 18:36:46,754 >> ***** Running training *****\n",
            "[INFO|trainer.py:1651] 2023-02-19 18:36:46,754 >>   Num examples = 711\n",
            "[INFO|trainer.py:1652] 2023-02-19 18:36:46,754 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1653] 2023-02-19 18:36:46,754 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1654] 2023-02-19 18:36:46,754 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1655] 2023-02-19 18:36:46,754 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1656] 2023-02-19 18:36:46,754 >>   Total optimization steps = 450\n",
            "[INFO|trainer.py:1657] 2023-02-19 18:36:46,755 >>   Number of trainable parameters = 192018434\n",
            " 10% 45/450 [00:35<04:33,  1.48it/s][INFO|trainer.py:710] 2023-02-19 18:37:22,012 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:37:22,014 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:37:22,014 >>   Num examples = 109\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:37:22,014 >>   Batch size = 8\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 14.58it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:01,  9.15it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:00,  8.18it/s]\u001b[A\n",
            " 50% 7/14 [00:00<00:00,  7.90it/s]\u001b[A\n",
            " 57% 8/14 [00:00<00:00,  7.73it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:00,  7.59it/s]\u001b[A\n",
            " 71% 10/14 [00:01<00:00,  7.51it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  7.45it/s]\u001b[A\n",
            " 86% 12/14 [00:01<00:00,  7.41it/s]\u001b[A\n",
            " 93% 13/14 [00:01<00:00,  7.37it/s]\u001b[AINFO:utils_qa:Post-processing 109 example predictions split into 109 features.\n",
            "\n",
            "\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33% 36/109 [00:00<00:00, 356.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 109/109 [00:00<00:00, 366.76it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                    \n",
            "\u001b[A{'eval_exact_match': 8.256880733944953, 'eval_f1': 44.05122408804237, 'eval_runtime': 1.8821, 'eval_samples_per_second': 57.914, 'eval_steps_per_second': 7.438, 'epoch': 1.0}\n",
            " 10% 45/450 [00:37<04:33,  1.48it/s]\n",
            "100% 14/14 [00:02<00:00,  7.37it/s]\u001b[A\n",
            " 20% 90/450 [01:12<03:58,  1.51it/s][INFO|trainer.py:710] 2023-02-19 18:37:58,881 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:37:58,883 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:37:58,883 >>   Num examples = 109\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:37:58,883 >>   Batch size = 8\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 14.64it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:01,  9.22it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:00,  8.25it/s]\u001b[A\n",
            " 50% 7/14 [00:00<00:00,  7.97it/s]\u001b[A\n",
            " 57% 8/14 [00:00<00:00,  7.78it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:00,  7.66it/s]\u001b[A\n",
            " 71% 10/14 [00:01<00:00,  7.57it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  7.46it/s]\u001b[A\n",
            " 86% 12/14 [00:01<00:00,  7.41it/s]\u001b[A\n",
            " 93% 13/14 [00:01<00:00,  7.34it/s]\u001b[AINFO:utils_qa:Post-processing 109 example predictions split into 109 features.\n",
            "\n",
            "\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30% 33/109 [00:00<00:00, 327.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 61% 67/109 [00:00<00:00, 334.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 109/109 [00:00<00:00, 336.27it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                    \n",
            "\u001b[A{'eval_exact_match': 9.174311926605505, 'eval_f1': 46.28518684265739, 'eval_runtime': 1.873, 'eval_samples_per_second': 58.194, 'eval_steps_per_second': 7.475, 'epoch': 2.0}\n",
            " 20% 90/450 [01:14<03:58,  1.51it/s]\n",
            "100% 14/14 [00:02<00:00,  7.34it/s]\u001b[A\n",
            " 30% 135/450 [01:49<03:30,  1.50it/s][INFO|trainer.py:710] 2023-02-19 18:38:35,783 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:38:35,784 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:38:35,785 >>   Num examples = 109\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:38:35,785 >>   Batch size = 8\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 14.64it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:01,  9.18it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:00,  8.23it/s]\u001b[A\n",
            " 50% 7/14 [00:00<00:00,  7.95it/s]\u001b[A\n",
            " 57% 8/14 [00:00<00:00,  7.77it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:00,  7.62it/s]\u001b[A\n",
            " 71% 10/14 [00:01<00:00,  7.54it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  7.46it/s]\u001b[A\n",
            " 86% 12/14 [00:01<00:00,  7.42it/s]\u001b[A\n",
            " 93% 13/14 [00:01<00:00,  7.39it/s]\u001b[AINFO:utils_qa:Post-processing 109 example predictions split into 109 features.\n",
            "\n",
            "\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33% 36/109 [00:00<00:00, 356.67it/s]\u001b[A\u001b[A\n",
            "\n",
            " 66% 72/109 [00:00<00:00, 357.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 109/109 [00:00<00:00, 358.72it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                     \n",
            "\u001b[A{'eval_exact_match': 11.009174311926605, 'eval_f1': 47.1330055319911, 'eval_runtime': 1.8737, 'eval_samples_per_second': 58.174, 'eval_steps_per_second': 7.472, 'epoch': 3.0}\n",
            " 30% 135/450 [01:51<03:30,  1.50it/s]\n",
            "100% 14/14 [00:02<00:00,  7.39it/s]\u001b[A\n",
            " 40% 180/450 [02:26<03:01,  1.49it/s][INFO|trainer.py:710] 2023-02-19 18:39:12,892 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:39:12,894 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:39:12,894 >>   Num examples = 109\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:39:12,894 >>   Batch size = 8\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 14.69it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:01,  9.15it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:00,  8.20it/s]\u001b[A\n",
            " 50% 7/14 [00:00<00:00,  7.94it/s]\u001b[A\n",
            " 57% 8/14 [00:00<00:00,  7.76it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:00,  7.64it/s]\u001b[A\n",
            " 71% 10/14 [00:01<00:00,  7.53it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  7.48it/s]\u001b[A\n",
            " 86% 12/14 [00:01<00:00,  7.44it/s]\u001b[A\n",
            " 93% 13/14 [00:01<00:00,  7.40it/s]\u001b[AINFO:utils_qa:Post-processing 109 example predictions split into 109 features.\n",
            "\n",
            "\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33% 36/109 [00:00<00:00, 359.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 66% 72/109 [00:00<00:00, 318.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 109/109 [00:00<00:00, 334.50it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                     \n",
            "\u001b[A{'eval_exact_match': 10.091743119266056, 'eval_f1': 45.011124808347205, 'eval_runtime': 1.8748, 'eval_samples_per_second': 58.14, 'eval_steps_per_second': 7.467, 'epoch': 4.0}\n",
            " 40% 180/450 [02:28<03:01,  1.49it/s]\n",
            "100% 14/14 [00:02<00:00,  7.40it/s]\u001b[A\n",
            " 50% 225/450 [03:03<02:29,  1.50it/s][INFO|trainer.py:710] 2023-02-19 18:39:49,811 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:39:49,813 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:39:49,813 >>   Num examples = 109\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:39:49,813 >>   Batch size = 8\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 14.53it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:01,  9.10it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:00,  8.18it/s]\u001b[A\n",
            " 50% 7/14 [00:00<00:00,  7.95it/s]\u001b[A\n",
            " 57% 8/14 [00:00<00:00,  7.78it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:00,  7.62it/s]\u001b[A\n",
            " 71% 10/14 [00:01<00:00,  7.54it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  7.48it/s]\u001b[A\n",
            " 86% 12/14 [00:01<00:00,  7.43it/s]\u001b[A\n",
            " 93% 13/14 [00:01<00:00,  7.39it/s]\u001b[AINFO:utils_qa:Post-processing 109 example predictions split into 109 features.\n",
            "\n",
            "\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33% 36/109 [00:00<00:00, 352.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67% 73/109 [00:00<00:00, 356.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 109/109 [00:00<00:00, 350.73it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                     \n",
            "\u001b[A{'eval_exact_match': 16.513761467889907, 'eval_f1': 49.969372679643165, 'eval_runtime': 1.8768, 'eval_samples_per_second': 58.076, 'eval_steps_per_second': 7.459, 'epoch': 5.0}\n",
            " 50% 225/450 [03:05<02:29,  1.50it/s]\n",
            "100% 14/14 [00:02<00:00,  7.39it/s]\u001b[A\n",
            " 60% 270/450 [03:40<02:01,  1.48it/s][INFO|trainer.py:710] 2023-02-19 18:40:27,041 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:40:27,043 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:40:27,043 >>   Num examples = 109\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:40:27,043 >>   Batch size = 8\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 14.53it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:01,  9.19it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:00,  8.23it/s]\u001b[A\n",
            " 50% 7/14 [00:00<00:00,  7.97it/s]\u001b[A\n",
            " 57% 8/14 [00:00<00:00,  7.77it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:00,  7.64it/s]\u001b[A\n",
            " 71% 10/14 [00:01<00:00,  7.56it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  7.50it/s]\u001b[A\n",
            " 86% 12/14 [00:01<00:00,  7.44it/s]\u001b[A\n",
            " 93% 13/14 [00:01<00:00,  7.41it/s]\u001b[AINFO:utils_qa:Post-processing 109 example predictions split into 109 features.\n",
            "\n",
            "\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 37/109 [00:00<00:00, 365.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 109/109 [00:00<00:00, 362.72it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                     \n",
            "\u001b[A{'eval_exact_match': 18.34862385321101, 'eval_f1': 53.99601042660808, 'eval_runtime': 1.8697, 'eval_samples_per_second': 58.297, 'eval_steps_per_second': 7.488, 'epoch': 6.0}\n",
            " 60% 270/450 [03:42<02:01,  1.48it/s]\n",
            "100% 14/14 [00:02<00:00,  7.41it/s]\u001b[A\n",
            " 70% 315/450 [04:17<01:31,  1.47it/s][INFO|trainer.py:710] 2023-02-19 18:41:03,989 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:41:03,991 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:41:03,991 >>   Num examples = 109\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:41:03,991 >>   Batch size = 8\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 14.56it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:01,  9.12it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:00,  8.16it/s]\u001b[A\n",
            " 50% 7/14 [00:00<00:00,  7.90it/s]\u001b[A\n",
            " 57% 8/14 [00:00<00:00,  7.74it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:00,  7.61it/s]\u001b[A\n",
            " 71% 10/14 [00:01<00:00,  7.51it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  7.44it/s]\u001b[A\n",
            " 86% 12/14 [00:01<00:00,  7.41it/s]\u001b[A\n",
            " 93% 13/14 [00:01<00:00,  7.38it/s]\u001b[AINFO:utils_qa:Post-processing 109 example predictions split into 109 features.\n",
            "\n",
            "\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28% 31/109 [00:00<00:00, 305.50it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59% 64/109 [00:00<00:00, 317.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 109/109 [00:00<00:00, 320.63it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                     \n",
            "\u001b[A{'eval_exact_match': 18.34862385321101, 'eval_f1': 53.43614021189346, 'eval_runtime': 1.8808, 'eval_samples_per_second': 57.955, 'eval_steps_per_second': 7.444, 'epoch': 7.0}\n",
            " 70% 315/450 [04:19<01:31,  1.47it/s]\n",
            "100% 14/14 [00:02<00:00,  7.38it/s]\u001b[A\n",
            " 80% 360/450 [04:54<00:59,  1.50it/s][INFO|trainer.py:710] 2023-02-19 18:41:40,869 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:41:40,871 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:41:40,871 >>   Num examples = 109\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:41:40,871 >>   Batch size = 8\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 14.69it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:01,  9.25it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:00,  8.26it/s]\u001b[A\n",
            " 50% 7/14 [00:00<00:00,  7.99it/s]\u001b[A\n",
            " 57% 8/14 [00:00<00:00,  7.80it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:00,  7.67it/s]\u001b[A\n",
            " 71% 10/14 [00:01<00:00,  7.58it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  7.51it/s]\u001b[A\n",
            " 86% 12/14 [00:01<00:00,  7.43it/s]\u001b[A\n",
            " 93% 13/14 [00:01<00:00,  7.40it/s]\u001b[AINFO:utils_qa:Post-processing 109 example predictions split into 109 features.\n",
            "\n",
            "\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 37/109 [00:00<00:00, 362.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 109/109 [00:00<00:00, 353.56it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                     \n",
            "\u001b[A{'eval_exact_match': 20.18348623853211, 'eval_f1': 55.834231281996615, 'eval_runtime': 1.8669, 'eval_samples_per_second': 58.385, 'eval_steps_per_second': 7.499, 'epoch': 8.0}\n",
            " 80% 360/450 [04:56<00:59,  1.50it/s]\n",
            "100% 14/14 [00:02<00:00,  7.40it/s]\u001b[A\n",
            " 90% 405/450 [05:30<00:30,  1.50it/s][INFO|trainer.py:710] 2023-02-19 18:42:17,737 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:42:17,738 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:42:17,738 >>   Num examples = 109\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:42:17,738 >>   Batch size = 8\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 14.67it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:01,  9.20it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:00,  8.24it/s]\u001b[A\n",
            " 50% 7/14 [00:00<00:00,  7.97it/s]\u001b[A\n",
            " 57% 8/14 [00:00<00:00,  7.81it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:00,  7.69it/s]\u001b[A\n",
            " 71% 10/14 [00:01<00:00,  7.59it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  7.52it/s]\u001b[A\n",
            " 86% 12/14 [00:01<00:00,  7.47it/s]\u001b[A\n",
            " 93% 13/14 [00:01<00:00,  7.41it/s]\u001b[AINFO:utils_qa:Post-processing 109 example predictions split into 109 features.\n",
            "\n",
            "\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 37/109 [00:00<00:00, 363.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 109/109 [00:00<00:00, 361.11it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                     \n",
            "\u001b[A{'eval_exact_match': 19.26605504587156, 'eval_f1': 53.27749811403829, 'eval_runtime': 1.866, 'eval_samples_per_second': 58.412, 'eval_steps_per_second': 7.502, 'epoch': 9.0}\n",
            " 90% 405/450 [05:33<00:30,  1.50it/s]\n",
            "100% 14/14 [00:02<00:00,  7.41it/s]\u001b[A\n",
            "100% 450/450 [06:07<00:00,  1.51it/s][INFO|trainer.py:710] 2023-02-19 18:42:54,580 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:42:54,582 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:42:54,582 >>   Num examples = 109\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:42:54,582 >>   Batch size = 8\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 14.61it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:01,  9.18it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:00,  8.20it/s]\u001b[A\n",
            " 50% 7/14 [00:00<00:00,  7.94it/s]\u001b[A\n",
            " 57% 8/14 [00:00<00:00,  7.78it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:00,  7.57it/s]\u001b[A\n",
            " 71% 10/14 [00:01<00:00,  7.50it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  7.45it/s]\u001b[A\n",
            " 86% 12/14 [00:01<00:00,  7.42it/s]\u001b[A\n",
            " 93% 13/14 [00:01<00:00,  7.39it/s]\u001b[AINFO:utils_qa:Post-processing 109 example predictions split into 109 features.\n",
            "\n",
            "\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34% 37/109 [00:00<00:00, 363.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 109/109 [00:00<00:00, 345.83it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "                                     \n",
            "\u001b[A{'eval_exact_match': 18.34862385321101, 'eval_f1': 53.286887306007245, 'eval_runtime': 1.8765, 'eval_samples_per_second': 58.087, 'eval_steps_per_second': 7.461, 'epoch': 10.0}\n",
            "100% 450/450 [06:10<00:00,  1.51it/s]\n",
            "100% 14/14 [00:02<00:00,  7.39it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1901] 2023-02-19 18:42:56,936 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 370.1814, 'train_samples_per_second': 19.207, 'train_steps_per_second': 1.216, 'train_loss': 0.7095586480034722, 'epoch': 10.0}\n",
            "100% 450/450 [06:10<00:00,  1.22it/s]\n",
            "[INFO|trainer.py:2709] 2023-02-19 18:42:56,938 >> Saving model checkpoint to out\n",
            "[INFO|configuration_utils.py:453] 2023-02-19 18:42:56,939 >> Configuration saved in out/config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-19 18:42:59,286 >> Model weights saved in out/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-19 18:42:59,287 >> tokenizer config file saved in out/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-19 18:42:59,287 >> Special tokens file saved in out/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =     0.7096\n",
            "  train_runtime            = 0:06:10.18\n",
            "  train_samples            =        711\n",
            "  train_samples_per_second =     19.207\n",
            "  train_steps_per_second   =      1.216\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:710] 2023-02-19 18:42:59,389 >> The following columns in the evaluation set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:42:59,391 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:42:59,391 >>   Num examples = 109\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:42:59,391 >>   Batch size = 8\n",
            " 93% 13/14 [00:01<00:00,  7.41it/s]INFO:utils_qa:Post-processing 109 example predictions split into 109 features.\n",
            "\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\n",
            " 34% 37/109 [00:00<00:00, 364.12it/s]\u001b[A\n",
            "100% 109/109 [00:00<00:00, 374.73it/s]\n",
            "INFO:utils_qa:Saving predictions to out/eval_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/eval_nbest_predictions.json.\n",
            "100% 14/14 [00:02<00:00,  6.38it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       10.0\n",
            "  eval_exact_match        =    18.3486\n",
            "  eval_f1                 =    53.2869\n",
            "  eval_runtime            = 0:00:01.93\n",
            "  eval_samples            =        109\n",
            "  eval_samples_per_second =     56.401\n",
            "  eval_steps_per_second   =      7.244\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:710] 2023-02-19 18:43:01,772 >> The following columns in the test set don't have a corresponding argument in `FunnelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `FunnelForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2964] 2023-02-19 18:43:01,773 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2966] 2023-02-19 18:43:01,773 >>   Num examples = 238\n",
            "[INFO|trainer.py:2969] 2023-02-19 18:43:01,773 >>   Batch size = 8\n",
            "100% 30/30 [00:03<00:00,  7.83it/s]INFO:utils_qa:Post-processing 238 example predictions split into 238 features.\n",
            "\n",
            "  0% 0/238 [00:00<?, ?it/s]\u001b[A\n",
            " 15% 36/238 [00:00<00:00, 357.31it/s]\u001b[A\n",
            " 30% 72/238 [00:00<00:00, 358.14it/s]\u001b[A\n",
            " 45% 108/238 [00:00<00:00, 352.89it/s]\u001b[A\n",
            " 61% 146/238 [00:00<00:00, 361.54it/s]\u001b[A\n",
            " 77% 183/238 [00:00<00:00, 362.08it/s]\u001b[A\n",
            "100% 238/238 [00:00<00:00, 362.60it/s]\n",
            "INFO:utils_qa:Saving predictions to out/predict_predictions.json.\n",
            "INFO:utils_qa:Saving nbest_preds to out/predict_nbest_predictions.json.\n",
            "***** predict metrics *****\n",
            "  predict_samples         =        238\n",
            "  test_exact_match        =    16.3866\n",
            "  test_f1                 =     53.471\n",
            "  test_runtime            = 0:00:04.06\n",
            "  test_samples_per_second =     58.554\n",
            "  test_steps_per_second   =      7.381\n",
            "[INFO|modelcard.py:449] 2023-02-19 18:43:07,617 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'quran_qa.py secondary_task', 'type': 'quran_qa.py', 'config': 'secondary_task', 'split': 'validation', 'args': 'secondary_task'}}\n",
            "100% 30/30 [00:05<00:00,  5.25it/s]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}