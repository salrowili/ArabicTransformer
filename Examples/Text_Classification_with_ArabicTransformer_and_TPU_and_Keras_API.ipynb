{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification with ArabicTransformer and TPU and Keras API.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQvQLZH99PFw"
      },
      "source": [
        "**Text Classification with ArabicTransformer and TPU with Keras API**\n",
        "\n",
        "*   First, you need to activate TPU by going to Runtime-> Change RunTime Type -> TPU .\n",
        "\n",
        "*   This example was tested with HuggingFace Transformer Library version v4.11.2 . If you experience any issue roll back to this version.\n",
        "\n",
        "*   This example uses Keras API with TensorFlow. Keras API makes it easy for fine-tuning since you don't need to have Google Bucket like the case in native TensorFlow with TPU. \n",
        "\n",
        "*   In our paper, we use the original implementation of funnel transformer (PyTorch) (https://github.com/laiguokun/Funnel-Transformer) and V100 GPU, which is no longer provided for Google Colab Pro users. We will update you later on our modified code of the Funnel Transfomer library. However, in the meantime, you need to find the best hyperparameters here and don't rely on our setting in this notebook since the implementation is different from our paper. However, our current set of hyperparameters in this example is still close to what we reported in our paper. You may also get better results with our model than what we reported if you extend the grid search (:\n",
        "\n",
        "* You can easily run this code on GPU since TensorFlow and Keras API can work with GPU.\n",
        "\n",
        "* One disadvantage of this approach over PyTorchXLA, which we show in other examples, is the instability of the result for each run. If you run the same code with the same hyperparameters with the same seed, you may get different results. This is because the seed in TensorFlow is related to the graph itself. Try to solve this problem from your own side.\n",
        "\n",
        "\n",
        "*   This example is based on the GLUE fine-tuning task example from the hugging face team, but it can work with any text classification task and can be used to fine-tune any Arabic Language Model that was uploaded to HuggingFace Hub here https://huggingface.co/models . A text classification task is where we have a sentence and a label like sentiment analysis tasks. You just need to name the header of the first sentence that you need to classify to sentence1 and label to \"label\" column. If you want to classify two sentences, then name the first sentence as sentence1 and the other one to sentence2.\n",
        "\n",
        "\n",
        "*   We did not include language models that use pre-segmentation (FARASA), such as AraBERTv2, in the list of models below. You can do the pre-segmentation part from your own side using codes that AUB Mind published here https://github.com/aub-mind/arabert. Then use our code to fine-tune AraBERTv2 or similar models.\n",
        "\n",
        "*   If the model scale is changed (small, base, large) or the architecture is different (Funnel, BERT, ELECTRA, ALBERT), you need to change your hyperparameters. Evaluating all models using the same hyperparameters across different scales and architectures is bad practice to report results.\n",
        "\n",
        "*   We suggest you use this example to find the best hyperparameters and then use PyTorch XLA to report the result with the same hyperparameters that you find the best. This will lead to more reproducibility for your work. See these posts :\n",
        "\n",
        "https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras\n",
        "\n",
        "https://stackoverflow.com/questions/50659482/why-cant-i-get-reproducible-results-in-keras-even-though-i-set-the-random-seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaihe27TrDHX",
        "outputId": "baccbc4c-f715-438e-9a3e-611c09847f2a"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip3 install -e transformers \n",
        "!pip3 install -r transformers/examples/tensorflow/text-classification/requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 85569, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 85569 (delta 8), reused 17 (delta 3), pack-reused 85541\u001b[K\n",
            "Receiving objects: 100% (85569/85569), 68.50 MiB | 34.76 MiB/s, done.\n",
            "Resolving deltas: 100% (61486/61486), done.\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.62.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 74.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 71.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.12.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.12.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.0.dev0\n",
            "Collecting datasets>=1.1.3\n",
            "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 83.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r transformers/examples/tensorflow/text-classification/requirements.txt (line 3)) (3.17.3)\n",
            "Requirement already satisfied: tensorflow>=2.3 in /usr/local/lib/python3.7/dist-packages (from -r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (2.6.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (4.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (21.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (4.62.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 82.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (0.0.17)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (0.70.12.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 68.0 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 90.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (2.6.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (2.6.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (0.37.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (2.6.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.40.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 4)) (3.1.1)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 78.9 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 89.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r transformers/examples/tensorflow/text-classification/requirements.txt (line 1)) (2.8.2)\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, sentencepiece, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.12.1 fsspec-2021.10.0 multidict-5.1.0 sentencepiece-0.1.96 xxhash-2.0.2 yarl-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJsmWd35r8Xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aed2a4e-53eb-4bb2-9121-9bee1d158ab2"
      },
      "source": [
        "import pandas as pd\n",
        "!rm -r /content/data\n",
        "!mkdir -p data/raw/scarcasmv2\n",
        "!mkdir -p data/scarcasmv2\n",
        "!wget -O data/raw/scarcasmv2/dev.csv https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/testing_data.csv\n",
        "!wget -O data/raw/scarcasmv2/train.csv https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv\n",
        "df = pd.read_csv(r'data/raw/scarcasmv2/train.csv', header=0,escapechar='\\n',usecols = [0,2],names=[\"sentence1\", \"label\"])\n",
        "df.to_csv('data/scarcasmv2/train.csv',index=False)\n",
        "df.to_csv('data/scarcasmv2/train.tsv',sep='\\t',index=False)\n",
        "df = pd.read_csv(r'data/raw/scarcasmv2/dev.csv', header=0, escapechar='\\n',usecols = [0,2],names=[\"sentence1\", \"label\"])\n",
        "df.to_csv('data/scarcasmv2/dev.csv',index=False)\n",
        "df.to_csv('data/scarcasmv2/dev.tsv',sep='\\t',index=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/data': No such file or directory\n",
            "--2021-10-02 13:48:15--  https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/testing_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 585081 (571K) [text/plain]\n",
            "Saving to: ‘data/raw/scarcasmv2/dev.csv’\n",
            "\n",
            "data/raw/scarcasmv2 100%[===================>] 571.37K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-10-02 13:48:15 (13.6 MB/s) - ‘data/raw/scarcasmv2/dev.csv’ saved [585081/585081]\n",
            "\n",
            "--2021-10-02 13:48:16--  https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2393168 (2.3M) [text/plain]\n",
            "Saving to: ‘data/raw/scarcasmv2/train.csv’\n",
            "\n",
            "data/raw/scarcasmv2 100%[===================>]   2.28M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-10-02 13:48:16 (37.2 MB/s) - ‘data/raw/scarcasmv2/train.csv’ saved [2393168/2393168]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdjpXkYTQiTX"
      },
      "source": [
        "We need to add this line to TensorFlow code can accept pre-trained models that are in format of HuggingFace PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCWeI2YMwOGP"
      },
      "source": [
        "!sed -i '420i\\\\t          from_pt=True,' /content/transformers/examples/tensorflow/text-classification/run_text_classification.py"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhZCWHnXovP"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score,classification_report,accuracy_score\n",
        "def calc_scarcasm(y_pred,y_true):\n",
        "  y_pred=pd.read_csv(y_pred, sep='\\t',header=None,usecols=[1] )\n",
        "  y_true=pd.read_csv(y_true,usecols=[1],header=None)\n",
        "  print(\"Accur Score:\",accuracy_score(y_true, y_pred)*100)\n",
        "  print(\"F1 PN Score:\",f1_score(y_true, y_pred,labels=['NEG','POS'],average=\"macro\")*100)\n",
        "  print(\"########################### Full Report ###########################\")\n",
        "  print(classification_report(y_true, y_pred,digits=4,labels=['NEG','POS'] ))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Km3xMPqraxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc75596b-d01c-4b53-8514-457305eecdea"
      },
      "source": [
        "import os\n",
        "model= \"sultan/ArabicTransformer-small\" #@param [\"sultan/ArabicTransformer-small\",\"sultan/ArabicTransformer-intermediate\",\"sultan/ArabicTransformer-large\",\"aubmindlab/araelectra-base-discriminator\",\"asafaya/bert-base-arabic\",\"aubmindlab/bert-base-arabertv02\",\"kuisailab/albert-base-arabic\",\"aubmindlab/bert-large-arabertv02\"]\n",
        "task= \"scarcasmv2\" #@param [\"scarcasmv2\"]\n",
        "batch_size = 64 #@param {type:\"slider\", min:4, max:128, step:4}\n",
        "learning_rate = \"3e-5\"#@param [\"1e-4\",\"2e-4\", \"3e-4\", \"1e-5\",\"2e-5\",\"3e-5\",\"4e-5\",\"5e-5\",\"6e-5\",\"7e-5\",\"8e-5\",\"9e-5\"]\n",
        "epochs_num = 2 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "max_seq_length= \"256\" #@param [\"128\", \"256\", \"384\",\"512\"]\n",
        "os.environ['batch_size'] = str(batch_size)\n",
        "os.environ['learning_rate'] = str(learning_rate)\n",
        "os.environ['epochs_num'] = str(epochs_num)\n",
        "os.environ['task'] = str(task)\n",
        "os.environ['model'] = str(model)\n",
        "os.environ['max_seq_length'] = str(max_seq_length)\n",
        "!python transformers/examples/tensorflow/text-classification/run_text_classification.py --model_name_or_path $model \\\n",
        "--train_file data/$task/train.csv \\\n",
        "--test_file data/$task/dev.csv \\\n",
        "--output_dir output_dir/$task \\\n",
        "--overwrite_cache \\\n",
        "--overwrite_output_dir \\\n",
        "--logging_steps 1000000 \\\n",
        "--max_seq_length $max_seq_length \\\n",
        "--per_device_train_batch_size $batch_size \\\n",
        "--learning_rate $learning_rate \\\n",
        "--warmup_ratio 0.1 \\\n",
        "--num_train_epochs $epochs_num \\\n",
        "--save_steps 50000 \\\n",
        "--do_train \\\n",
        "--do_predict"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/02/2021 14:02:37 - INFO - __main__ - Training/evaluation parameters TFTrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gcp_project=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output_dir/scarcasmv2/runs/Oct02_14-02-37_9ff91d4f32e3,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=1000000,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "output_dir=output_dir/scarcasmv2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "poly_power=1.0,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output_dir/scarcasmv2,\n",
            "save_on_each_node=False,\n",
            "save_steps=50000,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_name=None,\n",
            "tpu_num_cores=None,\n",
            "tpu_zone=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xla=False,\n",
            "xpu_backend=None,\n",
            ")\n",
            "10/02/2021 14:02:37 - INFO - __main__ - Loading a local file for train: data/scarcasmv2/train.csv\n",
            "10/02/2021 14:02:37 - INFO - __main__ - Loading a local file for test: data/scarcasmv2/dev.csv\n",
            "10/02/2021 14:02:37 - WARNING - datasets.builder - Using custom data configuration default-11c1d1b827c4d4e3\n",
            "10/02/2021 14:02:37 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-11c1d1b827c4d4e3/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "100% 2/2 [00:00<00:00, 772.50it/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "100% 13/13 [00:00<00:00, 31.03ba/s]\n",
            "100% 3/3 [00:00<00:00, 40.85ba/s]\n",
            "2021-10-02 14:02:40.010275: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "10/02/2021 14:02:40 - INFO - absl - Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFFunnelForSequenceClassification: ['decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.1.attention.layer_norm.bias', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.k_head.weight']\n",
            "- This IS expected if you are initializing TFFunnelForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFFunnelForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFFunnelForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_hidden.bias', 'classifier.linear_out.weight', 'classifier.linear_out.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "10/02/2021 14:03:42 - INFO - __main__ - Padding all batches to max length because argument was set or we're on TPU.\n",
            "10/02/2021 14:03:46 - INFO - __main__ - Padding all batches to max length because argument was set or we're on TPU.\n",
            "Epoch 1/2\n",
            "  6/196 [..............................] - ETA: 18s - loss: 2.8440 - accuracy: 0.3490WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 11.8702s). Check your callbacks.\n",
            "10/02/2021 14:05:33 - WARNING - tensorflow - Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 11.8702s). Check your callbacks.\n",
            "196/196 [==============================] - 124s 100ms/step - loss: 0.8991 - accuracy: 0.6501\n",
            "Epoch 2/2\n",
            "196/196 [==============================] - 19s 99ms/step - loss: 0.5930 - accuracy: 0.7699\n",
            "10/02/2021 14:06:16 - INFO - __main__ - Doing predictions on test dataset...\n",
            "10/02/2021 14:06:18 - INFO - absl - TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(1, 154) dtype=int32>, <tf.Tensor 'cond_8/Identity_1:0' shape=(1, 154) dtype=int32>, <tf.Tensor 'cond_8/Identity_2:0' shape=(1, 154) dtype=int32>, <tf.Tensor 'cond_8/Identity_3:0' shape=(None,) dtype=int64>]\n",
            "10/02/2021 14:06:30 - INFO - __main__ - Wrote predictions to output_dir/scarcasmv2/test_results.txt!\n",
            "Computing prediction loss on test labels...\n",
            "Test loss: 0.7960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w7JCt7lQ3xl",
        "outputId": "60ddd1f1-ee9f-4597-d616-16c235bde33b"
      },
      "source": [
        "calc_scarcasm('/content/output_dir/scarcasmv2/test_results.txt','/content/data/scarcasmv2/dev.csv')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accur Score: 70.10996334555148\n",
            "F1 PN Score: 72.54026732668572\n",
            "########################### Full Report ###########################\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NEG     0.7656    0.8259    0.7946      1677\n",
            "         POS     0.6252    0.6904    0.6562       575\n",
            "\n",
            "   micro avg     0.7291    0.7913    0.7589      2252\n",
            "   macro avg     0.6954    0.7582    0.7254      2252\n",
            "weighted avg     0.7298    0.7913    0.7593      2252\n",
            "\n"
          ]
        }
      ]
    }
  ]
}